---
title: Neural Networks in Trading: Mask-Attention-Free Approach to Price Movement Forecasting
url: https://www.mql5.com/en/articles/15973
categories: Trading Systems, Expert Advisors, Machine Learning
relevance_score: 3
scraped_at: 2026-01-23T18:37:32.993156
---

[![](https://www.mql5.com/ff/sh/6zw0dkux8bqt7m6kz2/c0d1e95edf776bf88908b398733d0997.jpg)\\
MQL5 Channels - Messenger for traders\\
\\
Install the app and receive market analytics and trading tips.\\
\\
Download](https://www.mql5.com/ff/go?link=https://www.metatrader5.com/en/news/2270%3Futm_source=www.mql5.com%26utm_medium=display%26utm_term=messenger.for.traders%26utm_content=download.app%26utm_campaign=0524.mql5.channels&a=iuciwacmrxvmiibwyujliagqikizpsoo&s=268cbb13914c54b6c5c875db99b154944f6e0122b3400b54c9ac0d4f69f0f0d6&uid=&ref=https://www.mql5.com/en/articles/15973&id=bfogggabsofabcpxuzmgaibarmaxasdrj&fz_uniq=5069604131817064376)

MetaTrader 5 / Trading systems


### Introduction

We continue our exploration using the point cloud processing method. In the previous article, we introduced the _[SPFormer](https://www.mql5.com/en/articles/15928)_ method. Its authors developed a comprehensive algorithm based on the _Transformer_ architecture. Several layers of the Transformer decoder utilize a fixed number of object queries, enabling iterative processing of global features and direct object prediction. _SPFormer_ does not require post-processing to eliminate duplicates, as a one-to-one bipartite matching strategy is employed during training. Moreover, the object masks generated in the final layer are used to guide cross-attention.

However, the authors of the paper " [_Mask-Attention-Free Transformer for 3D Instance Segmentation_](https://www.mql5.com/go?link=https://arxiv.org/abs/2309.01692 "https://arxiv.org/abs/2309.01692")" note that current _Transformer_-based methods suffer from slow convergence. Through analysis of the baseline method, they identified that this issue may stem from the low quality of the initial masks. Specifically, the initial object masks are generated by mapping the similarity between initial object queries and per-point mask features. Poor-quality initial masks increase the complexity of training, thereby slowing down convergence.

To address the low completeness of initial masks, the authors propose a novel algorithm, _Mask-Attention-Free Transformer_ ( _MATF_), which abandons the mask attention design and instead introduces an auxiliary center regression task to guide cross-attention. To enable center regression, the authors developed a series of components that take point positions into account. First, they add a set of learnable positional queries, each representing the position of a corresponding content query. These query positions are densely distributed throughout the learning space. A constraint is also introduced to ensure that each query focuses on its local region. As a result, queries can effectively capture objects in the scene with higher distinctiveness, which is critical for reducing training complexity and accelerating convergence.

In addition, the authors of _MATF_ propose contextual relative position encoding for cross-attention. Compared to the attention mask used in previous works, this solution is more flexible, as attention weights are adjusted based on relative positions rather than a rigid mask. The query positions are updated iteratively to achieve more accurate representations.

The experimental results presented in the paper demonstrate that _MATF_ delivers superior performance across various datasets.

### 1\. The MATF algorithm

The _SPFormer_ algorithm represents a fully end-to-end pipeline that allows object queries to directly generate instance predictions. Using _Transformer_ decoders, a fixed number of object queries aggregate global object information from the analyzed point cloud. Moreover, _SPFormer_ leverages object masks to guide cross-attention, requiring queries to attend only to masked features. However, in the early stages of training, these masks are of low quality. This hampers performance in subsequent layers and increases the overall training complexity.

To address this, the authors of the _MAFT_ method introduce an auxiliary center regression task to guide instance segmentation. Initially, global positions ğ’« are selected from the raw point cloud, and global object features â„± are extracted via a backbone network. These can be voxels or _superpoints_. In addition to the content queries ğ’¬_0c_, the authors of _MAFT_ introduces a fixed number of positional queries ğ’¬_0p_, representing normalized object centers. While ğ’¬_0__p_ is initialized randomly, ğ’¬ _0_ _c_ starts with zero values. The core objective is to allow the positional queries to guide the corresponding contextual queries in cross-attention, followed by iterative refinement of both query sets to predict object centers, classes, and masks.

To effectively solve the object center regression task and improve the generation of initial object masks, the authors of _MAFT_ propose a series of architectural components that account for point positions.

Unlike prior approaches, an additional set of positional queries ğ’¬ _0p_ is introduced. Given that point ranges vary significantly across scenes, the initial position queries are stored in normalized form as learnable parameters, followed by a sigmoid activation function.

Notably, these initial position queries are densely distributed throughout the target space. Furthermore, each query aggregates objects from its corresponding local region. This design facilitates the ability of the initial queries to capture scene objects with high recall. It addresses the problem of low memorability caused by poor-quality initial instance masks and reduces the training complexity in subsequent layers.

In addition to absolute position encoding, _MAFT_ employs contextual relative position encoding in the cross-attention mechanism. To achieve this, relative positions ğ« between positional queries ğ’¬_tp_ and global positions ğ’« are first computed and then quantized into discrete integers ğ«'. These discrete relative positions are used as indices to retrieve corresponding values from a positional encoding table.

Next, the relative position encoding ğŸ _pos_ is multiplied with _Query_ ğŸ _q_ or _Key_ features ğŸ _k_ in the cross-attention module. The result is then added to the cross-attention weights, followed by a _Softmax_ function.

It is worth noting that relative positional encoding provides greater flexibility and robustness to errors compared to masked attention. In essence, it functions as a soft mask that flexibly adjusts attention weights rather than applying rigid masking. Another advantage is that it integrates semantic information and can selectively capture local context. This is achieved through the interaction between relative positions and semantic features.

As contextual queries in the decoder layers are continuously updated, maintaining fixed positional queries throughout decoding is suboptimal. Since the initial positional queries are static, it is beneficial to adapt them to the specific input scene in subsequent layers. To achieve this, the authors iteratively refine the positional queries based on the content queries. Specifically, an MLP is used to predict the center offset Î”p_t_ from the updated contextual query ğ’¬_t+1c_. This offset is then added to the previous positional query ğ’¬_tp._

The original visualization of the _MAFT_ method from the above paper is shown below.

![](https://c.mql5.com/2/141/4220126666360__1.png)

### 2\. Implementation in MQL5

Having explored the theoretical foundations of the _Mask-Attention-Free Transformer_ method, we now move on to the practical part of our article, where we implement our interpretation of the proposed approaches using _MQL5_. We begin by extending the _OpenCL_ program.

#### 2.1 Extending the OpenCL Program

We start by constructing the relative positional encoding algorithm. On one hand, the algorithm is relatively simple. We only need to calculate the distance between two points. Moreover, the authors compute the distance along each coordinate axis individually. On the other hand, the _MAFT_ authors perform quantization of the resulting offsets, which are then used to index into a learnable parameter table. We opted to slightly optimize the original solution. Our implementation is based on the assumption that the greatest influence comes from points located in close proximity to the query being analyzed. Following this logic, we first calculate the distance _S_ between two points in an _N_-dimensional space. And then compute the positional bias coefficient _kpb_ using the following formula:

![](https://c.mql5.com/2/141/2009251396227__1.png)

It is clear that the distance between any two points is always greater than or equal to 0. If the points coincide, the coefficient equals 1. As the distance increases, the relative positional encoding coefficient approaches 0.

The implementation of the proposed algorithm is provided in the _CalcPositionBias_ kernel. The kernel parameters include pointers to three global data buffers: 2 of them contain the input data. The third is intended for storing the results. Additionally, we specify the dimensionality of the feature vector for a single element.

Note that to correctly compute the distance between two vectors, they must be projected onto the same subspace. This implies that the feature vectors in both input tensors must have the same dimensionality.

```
__kernel void CalcPositionBias(__global const float *data1,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  __global const float *data2,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  __global float *result,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  const int dimension
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â {
Â Â  const size_t idx1 = get_global_id(0);
Â Â  const size_t idx2 = get_global_id(1);
Â Â  const size_t total1 = get_global_size(0);
Â Â  const size_t total2 = get_global_size(1);
```

We plan to launch the kernel in a two-dimensional space of tasks, each of which is equal to the number of elements in the corresponding tensor of the original data. In the kernel body, we immediately identify the current thread in both dimensions of the task space.

In the next step, we determine the offset into the data buffers.

```
Â Â  const int shift1 = idx1 * dimension;
Â Â  const int shift2 = idx2 * dimension;
Â Â  const int shift_out = idx1 * total2 + idx2;
```

After completing the preparatory stage, we proceed to the execution of calculations. Here, we first organize a loop for calculating the distance between the analyzed vectors.

```
Â Â  float res = 0;
Â Â  for(int i = 0; i < dimension; i++)
Â Â Â Â Â Â res = pow(data1[shift1 + i] - data2[shift2 + i], 2.0f);
Â Â  res = sqrt(res);
```

And then we compute the relative bias coefficient.

```
Â Â  res = 1.0f / exp(res);
Â Â  if(isnan(res) || isinf(res))
Â Â Â Â Â Â res = 0;
//---
Â Â  result[shift_out] = res;
Â Â }
```

We then write the calculated value to the corresponding element in the results buffer.

Note that none of the obtained coefficients are negative. This means that we are not masking out any elements in the input sequence. On the contrary, our implementation emphasizes elements that are spatially closest to the query.

At this stage, we have computed the relative positional bias coefficients. The next step is to integrate them into our cross-attention mechanism. However, before we proceed with the implementation, I would like to draw your attention to an important detail. Take a closer look at the authors' visualization of the _MAFT_ method shown above. Particularly pay attention the flow of scene representation information. What stands out is the authors' approach to positional encoding within the scene representation. Specifically, positional encoding is applied only to the _Key_ entities. The _Value_ entities remain unaffected by positional encoding. This appears to be a deliberate choice to ensure that attention weights are computed taking into account positional encoding. But at the same time, they do not distort the actual feature descriptors of the scene elements. Consequently, the _Key_ and _Value_ tensors must be generated from different sources. In practical terms, we must first generate the _Value_ tensor from the raw input representation. Then we add positional encoding to the original data. Only then can we obtain the _Key_ tensor.

Why am I highlighting this now? The reasoning above implies that we must separate the Key and Value entities into distinct tensors. We can design a new attention kernel that accounts for this architectural nuance. This approach will also allow us to avoid concatenating two tensors, which we previously had to perform.

To implement the attention algorithm, we will create a kernel called _MHPosBiasAttentionOut_. This kernel accepts a substantial list of global data buffers, many of which are familiar from our previous attention mechanism implementations. Additionally, we pass a pointer to the buffer of relative positional bias indicesÂ _pos\_bias_. We have also designed this kernel to optionally support standard attention computation without positional bias. This functionality can be enabled and disabled using the _use\_pos\_bias_ parameter.

```
__kernel void MHPosBiasAttentionOut(__global const float *q,Â Â Â Â Â Â Â Â  ///<[in] Matrix of Querys
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __global const float *k,Â Â Â Â Â Â Â Â  ///<[in] Matrix of Keys
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __global const float *v,Â Â Â Â Â Â Â Â  ///<[in] Matrix of Values
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __global float *score,Â Â Â Â Â Â Â Â Â Â  ///<[out] Matrix of Scores
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __global const float *pos_bias,Â Â ///<[in] Position Bias
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â __global float *out,Â Â Â Â Â Â Â Â Â Â Â Â  ///<[out] Matrix of attention
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int dimension,Â Â Â Â Â Â Â Â Â Â Â Â  ///< Dimension of Key
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int heads_kv,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int use_pos_bias
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  )
Â Â {
//---
Â Â  const int q_id = get_global_id(0);
Â Â  const int k_id = get_global_id(1);
Â Â  const int h = get_global_id(2);
Â Â  const int qunits = get_global_size(0);
Â Â  const int kunits = get_global_size(1);
Â Â  const int heads = get_global_size(2);
```

As before, we plan to run this kernel in a three-dimensional task space. The first two indicate the number of elements in the sequences being analyzed, and the third dimension of the task space indicates the number of attention heads used. The kernel algorithm starts by identifying the current thread in all three dimensions of the task space.

Next, we define all the necessary constants.

```
Â Â  const int h_kv = h % heads_kv;
Â Â  const int shift_q = dimension * (q_id * heads + h);
Â Â  const int shift_kv = dimension * (heads_kv * k_id + h_kv);
Â Â  const int shift_s = kunits * (q_id *Â Â heads + h) + k_id;
Â Â  const int shift_pb = q_id * kunits + k_id;
Â Â  const uint ls = min((uint)get_local_size(1), (uint)LOCAL_ARRAY_SIZE);
Â Â  float koef = sqrt((float)dimension);
Â Â  if(koef < 1)
Â Â Â Â Â Â koef = 1;
```

And then we create an array of data in local memory for exchanging data between threads of the local group.

```
Â Â  __local float temp[LOCAL_ARRAY_SIZE];
```

This completes the preparatory work stage, and we move directly to the calculations. The calculation process largely repeats the classical algorithm. We only add relative positional bias where necessary. The necessity of their use is controlled by the value of the _use\_pos\_bias_ parameter.

First, we calculate the sum of the exponential values of the attention coefficients. At the first stage, each thread of the local group calculates its part. Then it saves the result in the corresponding element of the local data array.

```
//--- sum of exp
Â Â  uint count = 0;
Â Â  if(k_id < ls)
Â Â Â Â  {
Â Â Â Â Â Â temp[k_id] = 0;
Â Â Â Â Â Â do
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(q_id >= (count * ls + k_id))
Â Â Â Â Â Â Â Â Â Â Â Â if((count * ls) < (kunits - k_id))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  float sum = 0;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  int sh_k = dimension * heads_kv * count * ls;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  for(int d = 0; d < dimension; d++)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â sum = q[shift_q + d] * k[shift_kv + d + sh_k];
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  sum = exp(sum / koef);
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  if(isnan(sum))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â sum = 0;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  temp[k_id] = temp[k_id] + sum + (use_pos_bias > 0 ? pos_bias[shift_pb + count * ls] : 0);
Â Â Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â  count++;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â while((count * ls + k_id) < kunits);
Â Â Â Â  }
Â Â  barrier(CLK_LOCAL_MEM_FENCE);
```

Please note that the sum we calculate must include the sum of the positional bias coefficients.

Next, we sum the value of the elements of the local array.

```
Â Â  count = min(ls, (uint)kunits);
//---
Â Â  do
Â Â Â Â  {
Â Â Â Â Â Â count = (count + 1) / 2;
Â Â Â Â Â Â if(k_id < ls)
Â Â Â Â Â Â Â Â  temp[k_id] += (k_id < count && (k_id + count) < kunits ? temp[k_id + count] : 0);
Â Â Â Â Â Â if(k_id + count < ls)
Â Â Â Â Â Â Â Â  temp[k_id + count] = 0;
Â Â Â Â Â Â barrier(CLK_LOCAL_MEM_FENCE);
Â Â Â Â  }
Â Â  while(count > 1);
```

After calculating the total sum, we determine and store the normalized values of the dependence coefficients taking into account the positional bias coefficients.

```
//--- score
Â Â  float sum = temp[0];
Â Â  float sc = 0;
Â Â  if(q_id >= (count * ls + k_id))
Â Â Â Â Â Â if(sum != 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  for(int d = 0; d < dimension; d++)
Â Â Â Â Â Â Â Â Â Â Â Â sc = q[shift_q + d] * k[shift_kv + d];
Â Â Â Â Â Â Â Â  sc = (exp(sc / koef) + (use_pos_bias > 0 ? pos_bias[shift_pb] : 0)) / sum;
Â Â Â Â Â Â Â Â  if(isnan(sc))
Â Â Â Â Â Â Â Â Â Â Â Â sc = 0;
Â Â Â Â Â Â Â Â }
Â Â  score[shift_s] = sc;
Â Â  barrier(CLK_LOCAL_MEM_FENCE);
```

The obtained attention coefficients allow us to calculate the final values of multi-headed attention for each element of the analyzed sequence.

```
//--- out
Â Â  for(int d = 0; d < dimension; d++)
Â Â Â Â  {
Â Â Â Â Â Â uint count = 0;
Â Â Â Â Â Â if(k_id < ls)
Â Â Â Â Â Â Â Â  do
Â Â Â Â Â Â Â Â Â Â  {
Â Â Â Â Â Â Â Â Â Â Â Â if((count * ls) < (kunits - k_id))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  int sh_v = 2 * dimension * heads_kv * count * ls;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  float sum =
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â v[shift_kv + d + sh_v] * (count == 0 ? sc : score[shift_s + count * ls]);
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  if(isnan(sum))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â sum = 0;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  temp[k_id] = (count > 0 ? temp[k_id] : 0) + sum;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â Â Â Â Â count++;
Â Â Â Â Â Â Â Â Â Â  }
Â Â Â Â Â Â Â Â  while((count * ls + k_id) < kunits);
Â Â Â Â Â Â barrier(CLK_LOCAL_MEM_FENCE);
Â Â Â Â Â Â //---
Â Â Â Â Â Â count = min(ls, (uint)kunits);
Â Â Â Â Â Â do
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  count = (count + 1) / 2;
Â Â Â Â Â Â Â Â  if(k_id < ls)
Â Â Â Â Â Â Â Â Â Â Â Â temp[k_id] += (k_id < count && (k_id + count) < kunits ? temp[k_id + count] : 0);
Â Â Â Â Â Â Â Â  if(k_id + count < ls)
Â Â Â Â Â Â Â Â Â Â Â Â temp[k_id + count] = 0;
Â Â Â Â Â Â Â Â  barrier(CLK_LOCAL_MEM_FENCE);
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â while(count > 1);
Â Â Â Â Â Â //---
Â Â Â Â Â Â out[shift_q + d] = temp[0];
Â Â Â Â  }
Â Â }
```

Next, we proceed to implement the backpropagation algorithm in the _MHPosBiasAttentionInsideGradients_ kernel. It is worth recalling that when distributing error gradients through a summation operation, the gradient is typically propagated in full to both summands. The use of a learning rate significantly less than 1 more than compensates for the potential overcounting of the error. Another key point to consider is that the calculation of relative positional bias coefficients is based solely on the actual spatial arrangement of points. These actually represent the raw input data. So, these computations are not influenced by model parameters. The calculation process itself includes no learnable parameters. Consequently, propagating gradients to the tensor of relative positional bias coefficients is illogical. Therefore, we exclude this step from the backpropagation process.

Taking these considerations into account, we arrive at a classical gradient distribution approach for the attention block. However, we developed a new kernel because, as discussed earlier, we separated the _Key_ and _Value_ entities into distinct data buffers. You can review the implementation of the _MHPosBiasAttentionInsideGradients_ backpropagation kernel in the attached file. With this, we conclude our work on the _OpenCL_ component.

#### 2.2 Creating the MAFT Class

The next phase of our work involves creating a new object that encapsulates our interpretation of the techniques proposed by the authors of the Mask-Attention-Free Transformer method. For this purpose, we introduce a new class named _CNeuronMAFT_.

The _MAFT_ algorithm is built upon the previously discussed _[SPFormer](https://www.mql5.com/en/articles/15928)_ architecture. Similarly, our implementation will leverage the groundwork laid in the _CNeuronSPFormer_ class. However, the scale and scope of changes render inheritance from that class impractical. As a result, our new object will inherit directly from the base fully connected layer class _CNeuronBaseOCL_. The structure of the new class is shown below.

```
class CNeuronMAFTÂ Â  : public CNeuronBaseOCL
Â Â {
protected:
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iWindow;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iUnits;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iHeads;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iSPWindow;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iSPUnits;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iSPHeads;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iWindowKey;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iLayers;
Â Â  uintÂ Â Â Â Â Â Â Â Â Â Â Â Â Â iLayersSP;
Â Â  //---
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cSuperPoints;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cQuery;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cQPosition;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cQKey;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cQValue;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cMHSelfAttentionOut;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cSelfAttentionOut;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cSPKey;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cSPValue;
Â Â  CArrayIntÂ Â Â Â Â Â Â Â  cScores;
Â Â  CArrayIntÂ Â Â Â Â Â Â Â  cPositionBias;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cMHCrossAttentionOut;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cCrossAttentionOut;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cResidual;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cFeedForward;
Â Â  CBufferFloatÂ Â Â Â Â Â cTempSP;
Â Â  CBufferFloatÂ Â Â Â Â Â cTempQ;
Â Â  CBufferFloatÂ Â Â Â Â Â cTempCrossK;
Â Â  CBufferFloatÂ Â Â Â Â Â cTempCrossV;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â CreateBuffers(void);
Â Â  virtual boolÂ Â Â Â Â Â CalcPositionBias(CBufferFloat *pos_q, CBufferFloat *pos_k, const int pos_bias,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int units,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int units_kv,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int dimension);
Â Â  virtual boolÂ Â Â Â Â Â AttentionOut(CNeuronBaseOCL *q, CNeuronBaseOCL *k, CNeuronBaseOCL *v,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int scores, CNeuronBaseOCL *out, const int pos_bias,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int units,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int heads,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int units_kv,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int heads_kv,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const int dimension,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â const bool use_pos_bias);
Â Â  virtual boolÂ Â Â Â Â Â AttentionInsideGradients(CNeuronBaseOCL *q, CNeuronBaseOCL *k, CNeuronBaseOCL *v,
Â Â Â Â Â Â Â Â  const int scores, CNeuronBaseOCL *out,
Â Â Â Â Â Â Â Â  const int units, const int heads,
Â Â Â Â Â Â Â Â  const int units_kv, const int heads_kv,
Â Â Â Â Â Â Â Â  const int dimension);
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â feedForward(CNeuronBaseOCL *NeuronOCL) override;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â calcInputGradients(CNeuronBaseOCL *NeuronOCL) override;
Â Â  virtual boolÂ Â Â Â Â Â updateInputWeights(CNeuronBaseOCL *NeuronOCL) override;

public:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  CNeuronMAFT(void) {};
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ~CNeuronMAFT(void) {};
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â uint window, uint window_key, uint units_count, uint heads,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â uint window_sp, uint units_sp, uint heads_sp,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â uint layers, uint layers_to_sp,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ENUM_OPTIMIZATION optimization_type, uint batch);
Â Â  //---
Â Â  virtual intÂ Â Â Â Â Â  Type(void) overrideÂ Â  constÂ Â  {Â Â return defNeuronMAFT; }
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Save(int const file_handle) override;
Â Â  virtual boolÂ Â Â Â Â Â Load(int const file_handle) override;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â WeightsUpdate(CNeuronBaseOCL *source, float tau) override;
Â Â  virtual voidÂ Â Â Â Â Â SetOpenCL(COpenCLMy *obj) override;
Â Â };
```

In the presented structure, we observe the familiar set of overridable virtual methods along with a large number of internal objects. Some of these internal components repeat those used previously, while others are entirely new. We will become familiar with the functionality of each as we proceed with the implementation of the _CNeuronMAFT_ class methods.

As before, all internal objects are declared statically, allowing us to leave the class constructor and destructor empty. Initialization of both inherited and newly declared components is handled in the _Init_ method.

```
bool CNeuronMAFT::Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  uint window, uint window_key, uint units_count,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  uint heads, uint window_sp, uint units_sp, uint heads_sp,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  uint layers, uint layers_to_sp,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ENUM_OPTIMIZATION optimization_type, uint batch)
Â Â {
Â Â  if(!CNeuronBaseOCL::Init(numOutputs, myIndex, open_cl, window * units_count, optimization_type, batch))
Â Â Â Â Â Â return false;
```

The parameters of this method include key constants defining the architecture of the object being created. It can be noticed here that the parameters of this method are completely borrowed from the relevant method of the _CNeuronSPFormer_ class. This is consistent with the inheritance-based design philosophy we are following. However, the actual logic of the method has not undergone major changes.

In the body of the method, we first call the same method of the parent class, which implements primary control over the received parameters and initializes the inherited objects. After that, we save the resulting constants in the internal variables of the class.

```
Â Â  iWindow = window;
Â Â  iUnits = units_count;
Â Â  iHeads = heads;
Â Â  iSPUnits = units_sp;
Â Â  iSPWindow = window_sp;
Â Â  iSPHeads = heads_sp;
Â Â  iWindowKey = window_key;
Â Â  iLayers = MathMax(layers, 1);
Â Â  iLayersSP = MathMax(layers_to_sp, 1);
```

The next step is to initialize the objects for generating learnable queries of objects and their positional encoding. The authors of the MAFT method proposed to initialize queries with zero values. We can do the same. To do this, we reset the query generation parameters.

```
Â Â  CNeuronBaseOCL *base = new CNeuronBaseOCL();
Â Â  if(!base)
Â Â Â Â Â Â return false;
Â Â  if(!base.Init(iWindow * iUnits, 0, OpenCL, 1, optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  CBufferFloat *buf = base.getOutput();
Â Â  if(!buf || !buf.BufferInit(1, 1) || !buf.BufferWrite())
Â Â Â Â Â Â return false;
Â Â  buf = base.getWeights();
Â Â  if(!buf || !buf.BufferInit(buf.Total(), 0) ||
Â Â Â Â Â Â !buf.BufferWrite())
Â Â Â Â Â Â return false;
Â Â  if(!cQuery.Add(base))
Â Â Â Â Â Â return false;
Â Â  base = new CNeuronBaseOCL();
Â Â  if(!base || !base.Init(0, 1, OpenCL, iWindow * iUnits, optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  if(!cQuery.Add(base))
Â Â Â Â Â Â return false;
```

We also add a learnable positional encoding initialized with random values.

```
Â Â  CNeuronLearnabledPE *pe = new CNeuronLearnabledPE();
Â Â  if(!pe || !pe.Init(0, 2, OpenCL, base.Neurons(), optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  if(!cQuery.Add(pe))
Â Â Â Â Â Â return false;
```

It should be said that positional coding goes as a separate flow of information through the entire _MAFT_ algorithm. Therefore, we will provide it as separate objects.

```
Â Â  if(!base || !base.Init(0, 3, OpenCL, pe.Neurons(), optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  if(!base.SetOutput(pe.getOutput()))
Â Â Â Â Â Â return false;
Â Â  if(!cQPosition.Add(base))
Â Â Â Â Â Â return false;
```

The next stage is primary data processing. And here we borrow the _Superpoint_ approach, which was presented in the _SPFormer_ method.

```
//--- Init SuperPoints
Â Â  int layer_id = 4;
Â Â  for(int r = 0; r < 4; r++)
Â Â Â Â  {
Â Â Â Â Â Â if(iSPUnits % 2 == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  iSPUnits /= 2;
Â Â Â Â Â Â Â Â  CResidualConv *residual = new CResidualConv();
Â Â Â Â Â Â Â Â  if(!residual)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!residual.Init(0, layer_id, OpenCL, 2 * iSPWindow, iSPWindow, iSPUnits, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSuperPoints.Add(residual))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â else
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  iSPUnits--;
Â Â Â Â Â Â Â Â  CNeuronConvOCL *conv = new CNeuronConvOCL();
Â Â Â Â Â Â Â Â  if(!conv.Init(0, layer_id, OpenCL, 2 * iSPWindow, iSPWindow, iSPWindow, iSPUnits, 1,
                                                                        optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSuperPoints.Add(conv))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â layer_id++;
Â Â Â Â  }
```

Please note here that the presented implementation allows the use of tensors of different dimensions for cross-attention. However, this is unacceptable for the proposed algorithm of relative positional offset coefficients. Therefore, we add a superpoint projection layer to the learnable query space.

```
Â Â  CNeuronConvOCL *conv = new CNeuronConvOCL();
Â Â  if(!conv.Init(0, layer_id, OpenCL, iSPWindow, iSPWindow, iWindow, iSPUnits, 1, optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  if(!cSuperPoints.Add(conv))
Â Â Â Â Â Â return false;
Â Â  layer_id++;
```

And we add a layer of positional encoding.

```
Â Â  pe = new CNeuronLearnabledPE();
Â Â  if(!pe || !pe.Init(0, layer_id, OpenCL, conv.Neurons(), optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  if(!cSuperPoints.Add(pe))
Â Â Â Â Â Â return false;
Â Â  layer_id++;
```

Please note that at this point, we have deviated a little from the original algorithm proposed by the authors of the _MAFT_ method. In their work, they used voxelization of a point cloud based on the original coordinates. Instead of this, we used fully learnable positional encoding, thereby allowing the model to learn the optimal positions of each element of the input sequence.

After completing the work on the primary processing of the source data, we organize a loop through the internal layers of the Decoder.

```
//--- Inside layers
Â Â  for(uint l = 0; l < iLayers; l++)
Â Â Â Â  {
Â Â Â Â Â Â //--- Self-Attention
Â Â Â Â Â Â //--- Query
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindowKey * iHeads, iUnits, 1,
                                                                              optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cQuery.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

Note here that the authors of MAFT use a classic layout: _Self-Attention_ -\> _Cross-Attention_ -\> _Feed Forward_. However, the authors of the _SPFormer_ method swapped _Self-Attention_ and _Cross-Attention._

First, we generate _Query_ entities. Then we add _Key_ and _Value_.

```
Â Â Â Â Â Â //--- Key
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindowKey * iHeads, iUnits, 1,
                                                                              optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cQKey.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
Â Â Â Â Â Â //--- Value
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindowKey * iHeads, iUnits, 1,
                                                                               optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cQValue.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

In this case, we expect to use a small number of learnable queries. Therefore, we do not reduce the number of heads for _Key-Value_ and generate new entities on each internal layer.

We pass the generated entities to the multi-headed attention block without using positional bias coefficients.

```
Â Â Â Â Â Â //--- Multy-Heads Attention Out
Â Â Â Â Â Â base = new CNeuronBaseOCL();
Â Â Â Â Â Â if(!base || !base.Init(0, layer_id, OpenCL, iWindowKey * iHeads * iUnits, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cMHSelfAttentionOut.Add(base))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

We add a layer of scaling the results of multi-headed attention.

```
Â Â Â Â Â Â //--- Self-Attention Out
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindowKey * iHeads, iWindowKey * iHeads, iWindow,
                                                                     iUnits, 1, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cSelfAttentionOut.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

At the end of the _Self-Attention_ block, following the classical _Transformer_ algorithm, we add a layer of residual connections.

```
Â Â Â Â Â Â //--- Residual
Â Â Â Â Â Â base = new CNeuronBaseOCL();
Â Â Â Â Â Â if(!base || !base.Init(0, layer_id, OpenCL, iWindow * iUnits, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cResidual.Add(base))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

Next, we build the objects of the cross-attention block. We start with the _Query_ entity tensor.

```
Â Â Â Â Â Â //--- Cross-Attention
Â Â Â Â Â Â //--- Query
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindowKey * iHeads, iUnits, 1,
                                                                              optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cQuery.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

Then we add tensors for the _Key_ and _Value_ entities. This time we follow the user's instructions to reduce attention heads and alternate layers.

```
Â Â Â Â Â Â if(l % iLayersSP == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  //--- Key
Â Â Â Â Â Â Â Â  conv = new CNeuronConvOCL();
Â Â Â Â Â Â Â Â  if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindowKey * iSPHeads, iSPUnits, 1,
                                                                                     optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSPKey.Add(conv))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  layer_id++;
Â Â Â Â Â Â Â Â  //--- Value
Â Â Â Â Â Â Â Â  conv = new CNeuronConvOCL();
Â Â Â Â Â Â Â Â  if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindowKey * iSPHeads, iSPUnits, 1,
                                                                                     optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSPValue.Add(conv))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  layer_id++;
Â Â Â Â Â Â Â Â }
```

We add a layer of results from multi-headed attention.

```
Â Â Â Â Â Â //--- Multy-Heads Attention Out
Â Â Â Â Â Â base = new CNeuronBaseOCL();
Â Â Â Â Â Â if(!base || !base.Init(0, layer_id, OpenCL, iWindowKey * iHeads * iUnits, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cMHCrossAttentionOut.Add(base))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

It is then scaled by adding residual connections.

```
Â Â Â Â Â Â //--- Cross-Attention Out
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindowKey * iHeads, iWindowKey * iHeads, iWindow,
                                                                    iUnits, 1, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cCrossAttentionOut.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
Â Â Â Â Â Â //--- Residual
Â Â Â Â Â Â base = new CNeuronBaseOCL();
Â Â Â Â Â Â if(!base || !base.Init(0, layer_id, OpenCL, iWindow * iUnits, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cResidual.Add(base))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

The decoder is completed by the _FeedForward_ block, to which we also add residual connections.

```
Â Â Â Â Â Â //--- Feed Forward
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, 4 * iWindow, iUnits, 1,
                                                                         optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â conv.SetActivationFunction(LReLU);
Â Â Â Â Â Â if(!cFeedForward.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, 4 * iWindow, 4 * iWindow, iWindow, iUnits, 1,
                                                                         optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cFeedForward.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
Â Â Â Â Â Â //--- Residual
Â Â Â Â Â Â base = new CNeuronBaseOCL();
Â Â Â Â Â Â if(!base || !base.Init(0, layer_id, OpenCL, iWindow * iUnits, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!base.SetGradient(conv.getGradient()))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cResidual.Add(base))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
```

Now, we just need to add MLP corrections for positional encoding of learnable queries.

```
Â Â Â Â Â Â //--- Delta position
Â Â Â Â Â Â conv = new CNeuronConvOCL();
Â Â Â Â Â Â if(!conv || !conv.Init(0, layer_id, OpenCL, iWindow, iWindow, iWindow, iUnits, 1, optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â conv.SetActivationFunction(SIGMOID);
Â Â Â Â Â Â if(!cQPosition.Add(conv))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
Â Â Â Â Â Â base = new CNeuronBaseOCL();
Â Â Â Â Â Â if(!base || !base.Init(0, layer_id, OpenCL, conv.Neurons(), optimization, iBatch))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!base.SetGradient(conv.getGradient()))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!cQPosition.Add(base))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â layer_id++;
Â Â Â Â  }
```

Then we move on to the next iteration of the loop, creating objects of the new internal layer of the Decoder.

After successful initialization of objects of all internal layers of the Decoder, we replace pointers to error gradient buffers and return the boolean result indicating success to the calling program.

```
Â Â  base = cResidual[iLayers * 3 - 1];
Â Â  if(!SetGradient(base.getGradient()))
Â Â Â Â Â Â return false;
//---
Â Â  SetOpenCL(OpenCL);
//---
Â Â  return true;
Â Â }
```

It should be added that the initialization of auxiliary data buffers is moved to a separate method _CreateBuffers_, which I suggest you study yourself.

The full implementation of this class and all its methods can be found in the attachment.

After initializing the internal objects, we move on to constructing a feed-forward pass algorithm in the _feedForward_ method. In the parameters of this method, we receive a pointer to the source data object.

```
bool CNeuronMAFT::feedForward(CNeuronBaseOCL *NeuronOCL)
Â Â {
//--- Superpoints
Â Â  CNeuronBaseOCL *superpoints = NeuronOCL;
Â Â  int total_sp = cSuperPoints.Total();
Â Â  for(int i = 0; i < total_sp; i++)
Â Â Â Â  {
Â Â Â Â Â Â if(!cSuperPoints[i] ||
Â Â Â Â Â Â Â Â  !((CNeuronBaseOCL*)cSuperPoints[i]).FeedForward(superpoints))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â superpoints = cSuperPoints[i];
Â Â Â Â  }
```

We immediately use the resulting object while generating superpoint features. For this we will use a nested model _cSuperPoints._

The last layer of this model is the positional encoding layer.

Next, we generate learnable queries with positional encoding.

```
//--- Query
Â Â  CNeuronBaseOCL *inputs = NULL;
Â Â  for(int i = 0; i < 2; i++)
Â Â Â Â  {
Â Â Â Â Â Â inputs = cQuery[i + 1];
Â Â Â Â Â Â if(!inputs ||
Â Â Â Â Â Â Â Â  !inputs.FeedForward(cQuery[i]))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â  }
```

Then, we create local variables to temporarily store pointers to objects.

```
Â Â  CNeuronBaseOCL *query = NULL, *key = NULL, *value = NULL, *base = NULL;
```

We organize a loop through the internal layers of the Decoder.

```
//--- Inside layers
Â Â  for(uint l = 0; l < iLayers; l++)
Â Â Â Â  {
Â Â Â Â Â Â //--- Self-Atention
Â Â Â Â Â Â query = cQuery[l * 2 + 3];
Â Â Â Â Â Â if(!query || !query.FeedForward(inputs))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â key = cQKey[l];
Â Â Â Â Â Â if(!key || !key.FeedForward(inputs))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â value = cQValue[l];
Â Â Â Â Â Â if(!value || !value.FeedForward(inputs))
Â Â Â Â Â Â Â Â  return false;
```

Here we first organize operations of the _Self-Attention_ block for the learnable queries with positional encoding. To do this, we first generate the necessary entities, which we pass to the multi-headed attention block.

```
Â Â Â Â Â Â if(!AttentionOut(query, key, value, cScores[l * 2], cMHSelfAttentionOut[l], -1,
                                   iUnits, iHeads, iUnits, iHeads, iWindowKey, false))
Â Â Â Â Â Â Â Â  return false;
```

Then we scale the obtained results and add residual connections.

```
Â Â Â Â Â Â base = cSelfAttentionOut[l];
Â Â Â Â Â Â if(!base || !base.FeedForward(cMHSelfAttentionOut[l]))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â value = cResidual[l * 3];
Â Â Â Â Â Â if(!value ||
Â Â Â Â Â Â Â Â  !SumAndNormilize(inputs.getOutput(), base.getOutput(), value.getOutput(), iWindow, true, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â inputs = value;
```

As a separate thread, we add positional coding.

```
Â Â Â Â Â Â value = cQPosition[l * 2];
Â Â Â Â Â Â if(!value ||
Â Â Â Â Â Â Â Â  !SumAndNormilize(inputs.getOutput(), value.getOutput(),inputs.getOutput(), iWindow, false, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â  return false;
```

After that we move on to the cross-attention block. But first, let's define the coefficients of relative positional bias.

```
Â Â Â Â Â Â //--- Calc Position bias
Â Â Â Â Â Â if(!CalcPositionBias(value.getOutput(),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ((CNeuronLearnabledPE*)superpoints).GetPE(), cPositionBias[l],
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  iUnits, iSPUnits, iWindow))
Â Â Â Â Â Â Â Â  return false;
```

Next, we generate a Query entity from the positional query tensor taking into account positional encoding.

```
Â Â Â Â Â Â //--- Cross-Attention
Â Â Â Â Â Â query = cQuery[l * 2 + 4];
Â Â Â Â Â Â if(!query || !query.FeedForward(inputs))
Â Â Â Â Â Â Â Â  return false;
```

As for the operations of the _Key_Â and _Value_ entities are full of nuances. First, new tensors are generated only when necessary.

```
Â Â Â Â Â Â key = cSPKey[l / iLayersSP];
Â Â Â Â Â Â value = cSPValue[l / iLayersSP];
Â Â Â Â Â Â if(l % iLayersSP == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(!key || !key.FeedForward(superpoints))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!value || !value.FeedForward(cSuperPoints[total_sp - 2]))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

Second, the _Key_ entity is generated from the data of the last _cSuperPoints_ layer, which contains positional encoding. To generate _Value_, we use the penultimate layer which does not have positional encoding.

We pass the resulting entities to the multi-headed attention block without using positional bias coefficients.

```
Â Â Â Â Â Â if(!AttentionOut(query, key, value, cScores[l * 2 + 1], cMHCrossAttentionOut[l], cPositionBias[l],
                                                  iUnits, iHeads, iSPUnits, iSPHeads, iWindowKey, true))
Â Â Â Â Â Â Â Â  return false;
```

After that we scale the obtained data and add residual connections.

```
Â Â Â Â Â Â base = cCrossAttentionOut[l];
Â Â Â Â Â Â if(!base || !base.FeedForward(cMHCrossAttentionOut[l]))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â value = cResidual[l * 3 + 1];
Â Â Â Â Â Â if(!value ||
Â Â Â Â Â Â Â Â  !SumAndNormilize(inputs.getOutput(), base.getOutput(), value.getOutput(), iWindow, true, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â inputs = value;
```

At the end of the Decoder, we pass the data through a FeedForward block followed by residual connections.

```
Â Â Â Â Â Â //--- Feed Forward
Â Â Â Â Â Â base = cFeedForward[l * 2];
Â Â Â Â Â Â if(!base || !base.FeedForward(inputs))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â base = cFeedForward[l * 2 + 1];
Â Â Â Â Â Â if(!base || !base.FeedForward(cFeedForward[l * 2]))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â value = cResidual[l * 3 + 2];
Â Â Â Â Â Â if(!value ||
Â Â Â Â Â Â Â Â  !SumAndNormilize(inputs.getOutput(), base.getOutput(), value.getOutput(), iWindow, true, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â inputs = value;
```

At this stage, we have completed the operations of one Decoder layer, but we still have to adjust the positional encoding data of the learnable queries. To do this, we will generate a position deviation based on the data received and add it to the existing values.

```
Â Â Â Â Â Â //--- Delta Query position
Â Â Â Â Â Â base = cQPosition[l * 2 + 1];
Â Â Â Â Â Â if(!base ||
Â Â Â Â Â Â Â Â  !base.FeedForward(inputs))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â value = cQPosition[(l + 1) * 2];
Â Â Â Â Â Â query = cQPosition[l * 2];
Â Â Â Â Â Â if(!value ||
Â Â Â Â Â Â Â Â  !SumAndNormilize(query.getOutput(), base.getOutput(), value.getOutput(), iWindow, false, 0,0,0,0.5f))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â  }
```

And now we can move on to performing the operations of the next inner layer of the Decoder.

After successful execution of operations of all internal layers of the Decoder, we receive the result in the form of enriched queries and their refined positions. We sum the 2 resulting tensors and pass them to the prediction head.

```
Â Â  value = cQPosition[iLayers * 2];
Â Â  if(!value ||
Â Â Â Â Â Â !SumAndNormilize(inputs.getOutput(), value.getOutput(), Output, iWindow, true, 0, 0, 0, 1))
Â Â Â Â Â Â return false;
//---
Â Â  return true;
Â Â }
```

The method returns a Boolean value indicating the success or failure of the initialization process.

With this, we complete the feed-forward pass implementation. We now proceed to develop the backpropagation algorithms, implemented in the _calcInputGradients_ and _updateInputWeights_ methods. The former is responsible for distributing the error gradients across all internal components in accordance with their contribution to the final output. The latter updates the model parameters.

As you know, gradient distribution is performed in strict reverse order relative to the information flow of the feed-forward pass. I encourage you to explore the implementation of these methods on your own.

The full implementation of this class and all its methods can be found in the attachment.

The architecture of the models used in this article, as well as all the programs for training and interaction with the environment, are entirely borrowed from our previous work. In fact, the only change we made to the encoder for the environmental state was modifying the identifier of a single layer. Therefore, we will not examine them in detail here. The complete code for all program classes used in preparing this article is also included in the attachment.

### 3\. Testing

In this article we got acquainted with the _MAFT_ method and implemented our vision of the proposed approaches in _MQL5_. We now proceed to evaluate the results of our work. The model will be trained on real historical data using the _MAFT_ framework, followed by testing the trained Actor policy.

As always, to train the models we use real historical data of the _EURUSD_ instrument, with the H1 timeframe, for the whole of 2023. All indicator parameters were set to their default values.

The model training procedure and related tools were carried over from our earlier articles.

The trained Actor policy was tested in the _MetaTrader 5_ Strategy Tester using historical data from January 2024. All other parameters remained unchanged. The test results are presented below.

![](https://c.mql5.com/2/141/330320335203__1.png)![](https://c.mql5.com/2/141/2423221815443__1.png)

The balance chart during the test period shows an upward trend, which is clearly a positive outcome. However, the model executed only 21 trades during the entire test period, of which 12 were profitable. Unfortunately, this limited number of trades does not allow for a conclusive evaluation of the model's effectiveness over longer periods.

### Conclusion

In this article, we discussed with the _Mask-Attention-Free Transformer_ ( _MAFT_) method and its application in algorithmic trading. Unlike traditional _Transformer_ architectures, _MAFT_ offers higher computational efficiency by eliminating the need for data masking and accelerating sequence processing.

The test results confirmed that MAFT can improve prediction accuracy while also reducing model training time.

#### References

- [Mask-Attention-Free Transformer for 3D Instance Segmentation](https://www.mql5.com/go?link=https://arxiv.org/abs/2309.01692 "Mask-Attention-Free Transformer for 3D Instance Segmentation")
- [Other articles from this series](https://www.mql5.com/en/search#!keyword=neural%20networks&author=DNG&method=2)

#### Programs used in the article

| # | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Research.mq5 | Expert Advisor | EA for collecting examples |
| 2 | ResearchRealORL.mq5 | Expert Advisor | EA for collecting examples using the Real-ORL method |
| 3 | Study.mq5 | Expert Advisor | Model training EA |
| 4 | Test.mq5 | Expert Advisor | Model testing EA |
| 5 | Trajectory.mqh | Class library | System state description structure |
| 6 | NeuroNet.mqh | Class library | A library of classes for creating a neural network |
| 7 | NeuroNet.cl | Library | OpenCL program code library |

Translated from Russian by MetaQuotes Ltd.

Original article: [https://www.mql5.com/ru/articles/15973](https://www.mql5.com/ru/articles/15973)

**Attached files** \|


[Download ZIP](https://www.mql5.com/en/articles/download/15973.zip "Download all attachments in the single ZIP archive")

[MQL5.zip](https://www.mql5.com/en/articles/download/15973/mql5.zip "Download MQL5.zip")(1878.59 KB)

**Warning:** All rights to these materials are reserved by MetaQuotes Ltd. Copying or reprinting of these materials in whole or in part is prohibited.

This article was written by a user of the site and reflects their personal views. MetaQuotes Ltd is not responsible for the accuracy of the information presented, nor for any consequences resulting from the use of the solutions, strategies or recommendations described.

#### Other articles by this author

- [Neural Networks in Trading: Hybrid Graph Sequence Models (GSM++)](https://www.mql5.com/en/articles/17279)
- [Neural Networks in Trading: Two-Dimensional Connection Space Models (Final Part)](https://www.mql5.com/en/articles/17241)
- [Neural Networks in Trading: Two-Dimensional Connection Space Models (Chimera)](https://www.mql5.com/en/articles/17210)
- [Neural Networks in Trading: Multi-Task Learning Based on the ResNeXt Model (Final Part)](https://www.mql5.com/en/articles/17157)
- [Neural Networks in Trading: Multi-Task Learning Based on the ResNeXt Model](https://www.mql5.com/en/articles/17142)
- [Neural Networks in Trading: Hierarchical Dual-Tower Transformer (Final Part)](https://www.mql5.com/en/articles/17104)
- [Neural Networks in Trading: Hierarchical Dual-Tower Transformer (Hidformer)](https://www.mql5.com/en/articles/17069)

**Last comments \|**
**[Go to discussion](https://www.mql5.com/en/forum/486360)**
(4)


![CapeCoddah](https://c.mql5.com/avatar/avatar_na2.png)

**[CapeCoddah](https://www.mql5.com/en/users/capecoddah)**
\|
15 May 2025 at 00:08

Hi Dmitriy,

It sems like your zip files have been incorrectly made.Â  I expected to see the source code listed in your boxÂ  instead this is what the zip contained.Â  It appears that each directory listed contains the files you have used in your various articles.Â  Could you provide a description of each or better yet, append the article number to each directory as appropriate.

Thanks

CapeCoddah

[![](https://c.mql5.com/3/464/4141078637813__1.png)](https://c.mql5.com/3/464/4141078637813.png "https://c.mql5.com/3/464/4141078637813.png")[![](https://c.mql5.com/3/464/4244013285421__1.png)](https://c.mql5.com/3/464/4244013285421.png "https://c.mql5.com/3/464/4244013285421.png")

![Dmitriy Gizlyk](https://c.mql5.com/avatar/2014/8/53E8CB77-1C48.png)

**[Dmitriy Gizlyk](https://www.mql5.com/en/users/dng)**
\|
15 May 2025 at 12:29

**CapeCoddah [#](https://www.mql5.com/en/forum/486360#comment_56704037):**

Hi Dmitriy,

It sems like your zip files have been incorrectly made.Â  I expected to see the source code listed in your boxÂ  instead this is what the zip contained.Â  It appears that each directory listed contains the files you have used in your various articles.Â  Could you provide a description of each or better yet, append the article number to each directory as appropriate.

Thanks

CapeCoddah

Hi CapeCoddah,

The zip file contains files from all series. The OpenCL program saved in "MQL5\\Experts\\NeuroNet\_DNG\\NeuroNet.cl". The library with all classes you can find in "MQL5\\Experts\\NeuroNet\_DNG\\NeuroNet.mqh". And model and experts referenced this article are located in the directory "MQL5\\Experts\\MAFT\\"

Regards,

Dmitriy.

![CapeCoddah](https://c.mql5.com/avatar/avatar_na2.png)

**[CapeCoddah](https://www.mql5.com/en/users/capecoddah)**
\|
17 May 2025 at 09:49

**Dmitriy Gizlyk [#](https://www.mql5.com/en/forum/486360#comment_56709037):**

Hi CapeCoddah,

The zip file contains files from all series. The OpenCL program saved in "MQL5\\Experts\\NeuroNet\_DNG\\NeuroNet.cl". The library with all classes you can find in "MQL5\\Experts\\NeuroNet\_DNG\\NeuroNet.mqh". And model and experts referenced this article are located in the directory "MQL5\\Experts\\MAFT\\"

Regards,

Dmitriy.

Hi Dmitriy,

Thanks for the prompt response.Â  I understand what you are saying but I think you misunderstood me.Â  How do I associate the subdirectory names with their respecitive article, either by name or by article number from which can search to find the article.

Cheers

CapeCoddah

![Dmitriy Gizlyk](https://c.mql5.com/avatar/2014/8/53E8CB77-1C48.png)

**[Dmitriy Gizlyk](https://www.mql5.com/en/users/dng)**
\|
17 May 2025 at 16:09

**CapeCoddah [#](https://www.mql5.com/en/forum/486360#comment_56721926):**

Hi Dmitriy,

Thanks for the prompt response.Â  I understand what you are saying but I think you misunderstood me.Â  How do I associate the subdirectory names with their respecitive article, either by name or by article number from which can search to find the article.

Cheers

CapeCoddah

ByÂ the name of the framework

![Custom Debugging and Profiling Tools for MQL5 Development (Part I): Advanced Logging](https://c.mql5.com/2/141/17933-custom-debugging-and-profiling-logo.png)[Custom Debugging and Profiling Tools for MQL5 Development (Part I): Advanced Logging](https://www.mql5.com/en/articles/17933)

Learn how to implement a powerful custom logging framework for MQL5 that goes beyond simple Print() statements by supporting severity levels, multiple output handlers, and automated file rotationâ€”all configurable onâ€theâ€fly. Integrate the singleton CLogger with ConsoleLogHandler and FileLogHandler to capture contextual, timestamped logs in both the Experts tab and persistent files. Streamline debugging and performance tracing in your Expert Advisors with clear, customizable log formats and centralized control.

![Price Action Analysis Toolkit Development (Part 22): Correlation Dashboard](https://c.mql5.com/2/141/18052-price-action-analysis-toolkit-logo.png)[Price Action Analysis Toolkit Development (Part 22): Correlation Dashboard](https://www.mql5.com/en/articles/18052)

This tool is a Correlation Dashboard that calculates and displays real-time correlation coefficients across multiple currency pairs. By visualizing how pairs move in relation to one another, it adds valuable context to your price-action analysis and helps you anticipate inter-market dynamics. Read on to explore its features and applications.

![Automating Trading Strategies in MQL5 (Part 17): Mastering the Grid-Mart Scalping Strategy with a Dynamic Dashboard](https://c.mql5.com/2/141/18038-automating-trading-strategies-logo.png)[Automating Trading Strategies in MQL5 (Part 17): Mastering the Grid-Mart Scalping Strategy with a Dynamic Dashboard](https://www.mql5.com/en/articles/18038)

In this article, we explore the Grid-Mart Scalping Strategy, automating it in MQL5 with a dynamic dashboard for real-time trading insights. We detail its grid-based Martingale logic and risk management features. We also guide backtesting and deployment for robust performance.

![African Buffalo Optimization (ABO)](https://c.mql5.com/2/97/African_Buffalo_Optimization___LOGO.png)[African Buffalo Optimization (ABO)](https://www.mql5.com/en/articles/16024)

The article presents the African Buffalo Optimization (ABO) algorithm, a metaheuristic approach developed in 2015 based on the unique behavior of these animals. The article describes in detail the stages of the algorithm implementation and its efficiency in finding solutions to complex problems, which makes it a valuable tool in the field of optimization.

[![](https://www.mql5.com/ff/sh/6xjc81sb5f2g45z9z2/01.png)Follow MQL5.community on social mediaWe publish the best technical materials from experts â€“ free from advertising and irrelevant contentLearn more](https://www.mql5.com/ff/go?link=https://www.mql5.com/en/forum/455636%3Futm_source=www.mql5.com%26utm_medium=display%26utm_content=follow.channel%26utm_campaign=AAA380.mql5.socials&a=yexgeaiatphxecqagtoxizolvboismyb&s=4e531fd1f983c26570e2dac7588b735354f2f9e0aea561427c030e4a1d2f060b&uid=&ref=https://www.mql5.com/en/articles/15973&id=wdausxxqrpvhekbwjrjlhqjghyhesrqqau&fz_uniq=5069604131817064376)

![MQL5 - Language of trade strategies built-in the MetaTrader 5 client terminal](https://c.mql5.com/i/registerlandings/logo-2.png)

You are missing trading opportunities:

- Free trading apps
- Over 8,000 signals for copying
- Economic news for exploring financial markets

RegistrationLog in

latin characters without spaces

a password will be sent to this email

An error occurred


- [Log in With Google](https://www.mql5.com/en/auth_oauth2?provider=Google&amp;return=popup&amp;reg=1)

You agree to [website policy](https://www.mql5.com/en/about/privacy) and [terms of use](https://www.mql5.com/en/about/terms)

If you do not have an account, please [register](https://www.mql5.com/en/auth_register)

Allow the use of cookies to log in to the MQL5.com website.

Please enable the necessary setting in your browser, otherwise you will not be able to log in.

[Forgot your login/password?](https://www.mql5.com/en/auth_forgotten?return=popup)

- [Log in With Google](https://www.mql5.com/en/auth_oauth2?provider=Google&amp;return=popup)

This website uses cookies. Learn more about our [Cookies Policy](https://www.mql5.com/en/about/cookies).