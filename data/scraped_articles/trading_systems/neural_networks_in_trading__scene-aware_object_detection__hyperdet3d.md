---
title: Neural Networks in Trading: Scene-Aware Object Detection (HyperDet3D)
url: https://www.mql5.com/en/articles/15859
categories: Trading Systems, Expert Advisors, Machine Learning
relevance_score: 3
scraped_at: 2026-01-23T18:38:07.296604
---

[![](https://www.mql5.com/ff/sh/7h2yc16rtqsn2m6kz2/c0d1e95edf776bf88908b398733d0997.jpg)\\
MQL5 Channels - Market analysis\\
\\
Dozens of channels, thousands of subscribers and daily updates. Learn more about trading.\\
\\
Download](https://www.mql5.com/ff/go?link=https://www.metatrader5.com/en/news/2270%3Futm_source=www.mql5.com%26utm_medium=display%26utm_term=messenger.for.traders%26utm_content=download.app%26utm_campaign=0524.mql5.channels&a=glufvbpblsoxonicqfngsyuzwfebnilr&s=103cc3ab372a16872ca1698fc86368ffe3b3eaa21b59b4006d5c6c10f48ad545&uid=&ref=https://www.mql5.com/en/articles/15859&id=bfogggabsofabcpxuzmgaibarmaxasdrj&fz_uniq=5069612210650548189)

MetaTrader 5 / Trading systems


### Introduction

In recent years, object detection has garnered significant attention. Based on feature learning and volumetric convolution, _[PointNet++](https://www.mql5.com/en/articles/15789)_ emphasizes local geometry, elegantly analyzing raw point clouds. This has led to its widespread adoption as a backbone network in various object detection models.

However, the attributes of similar objects can be ambiguous, which degrades model performance. As a result, the model's applicability becomes limited, or its architecture must be made more complex. The authors of the paper " _[HyperDet3D: Learning a Scene-conditioned 3D Object Detector](https://www.mql5.com/go?link=https://arxiv.org/pdf/2204.05599 "https://arxiv.org/pdf/2204.05599")_" observed that scene-level information provides prior knowledge that helps resolve ambiguity in object attribute interpretation. This, in turn, prevents illogical detection outcomes from a scene understanding perspective.

The paper introduces the _HyperDet3D_ algorithm for _3D_ object detection in point clouds, which uses a hypernetwork-based architecture. _HyperDet3D_ learns scene-conditioned information and incorporates scene-level knowledge into the network parameters. This allows the _3D_ object detector to dynamically adapt to varying input data. Specifically, scene-conditioned knowledge can be decomposed into two levels: scene-invariant information and scene-specific information.

To capture scene-invariant knowledge, the authors propose learning embeddings utilized by the hypernetwork and iteratively updated as the model is trained on diverse scenes. This scene-invariant information is typically abstracted from the characteristics of the training data and can be leveraged by the detector during inference.

Moreover, since conventional detectors maintain a fixed set of parameters when detecting objects across different scenes, the authors of _HyperDet3D_ propose integrating scene-specific information to adapt the detector to a particular scene at inference time. This is achieved by analyzing how closely the current scene aligns withâ€”or deviates fromâ€”the general representation, using the specific input as a query.

The paper introduces a novel module structure called _Multi-head Scene-Conditioned Attention_ ( _MSA_). _MSA_Â enables the aggregation of prior knowledge with candidate object features, thereby facilitating more effective object detection.

### 1\. The HyperDet3D Algorithm

The _HyperDet3D_ model includes three main components:

- A backbone Encoder
- An object Decoder layer
- A detection head

The input point cloud is first processed by the backbone, which downsamples the points to generate initial object candidates and coarsely extracts their features using hierarchical architectures. The authors propose using _PointNet++_ as the backbone network.

Next, the object decoder layers refine the candidate features, integrating scene-conditioned prior knowledge into the object-level representations. The detection head then regresses bounding boxes based on the locations and refined features of these candidate objects.

To enable _HyperDet3D_ to be aware of scene-level meta-information, the authors introduce a _HyperNetwork_, which is a neural network used to parameterize the trainable parameters of the primary network. Unlike standard deep neural networks that maintain fixed parameters during inference, hypernetworks offer flexibility by adapting parameters based on the input data.

HyperDet3D applies a scene-conditioned hypernetwork to incorporate prior knowledge into the parameters of the Transformer decoder layers. This allows the detection network to dynamically adapt to varying input scenes. The key idea is to enrich the object representation ğ’ formed from the candidate set generated by the backbone encoder, using prior knowledge parameterized by ğ‘¾ and provided by scene-conditioned hypernetworks.

The parameters generated by the scene-conditioned hypernetworks are divided into scene-invariant and scene-specific components.

To obtain scene-invariant knowledge, the authors propose training a set of _n_ scene-agnostic embedding vectors ğ’_a,_ which are then consumed by the hypernetwork. The output of this hypernetwork is a weight matrix ğ‘¾ _a_, which parameterizes the scene-invariant prior knowledge.

Since object properties are refined iteratively through a sequence of decoder layers, they can be progressively fused with the outputs of the scene-invariant hypernetwork. This network abstracts prior knowledge across diverse _3D_ scenes. As a result, _HyperDet3D_ not only maintains generalized scene-conditioned knowledge across all decoder levels but also conserves computational resources by sharing knowledge through rich feature hierarchies.

To obtain scene-specific knowledge, the model learns a set of embeddings ğ’_s_Â analogous to ğ’ _a_. But in this case,Â ğ’_s_Â should capture information unique to individual scenes. This is achieved through a cross-attention block, where the embedding of the current scene is compared against the learned scene-specific embeddingsÂ ğ’_s_.Â Through the attention mechanism, the model assesses how well ğ’_s_Â aligns with the current scene (or how much it deviates) in the embedding space.

An official visualization of the _HyperDet3D_ method is provided below.

![](https://c.mql5.com/2/135/2614214673656__1.png)

### 2\. Implementation in MQL5

After considering the theoretical aspects of the _HyperDet3D_ method, we move on to the practical part of our article, in which we implement our vision of the proposed approaches.

Let us say up front: we have a significant amount of work ahead. To manage this efficiently, we will divide the implementation into several logical modules. So, let's roll up our sleeves and get started.

#### 2.1 Scene-Specific Knowledge Module

We begin by building the module responsible for learning scene-specific knowledge. As discussed in the theoretical section, cross-attention is used to match the current scene with the scene-specific knowledge embeddings. Accordingly, we will define a new class, _CNeuronSceneSpecific_, as a subclass of the cross-attention block _CNeuronMLCrossAttentionMLKV_. The structure of the new class is shown below.

```
class CNeuronSceneSpecific :Â Â public CNeuronMLCrossAttentionMLKV
Â Â {
protected:
Â Â  CNeuronBaseOCLÂ Â Â Â cOne;
Â Â  CNeuronBaseOCLÂ Â Â Â cSceneSpecificKnowledge;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â feedForward(CNeuronBaseOCL *NeuronOCL) override;
Â Â  virtual boolÂ Â Â Â Â Â feedForward(CNeuronBaseOCL *NeuronOCL, CBufferFloat *Context) override
                                 { return feedForward(NeuronOCL); }
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â calcInputGradients(CNeuronBaseOCL *NeuronOCL, CBufferFloat *SecondInput,
                                                                   CBufferFloat *SecondGradient,
                                                        ENUM_ACTIVATION SecondActivation = None) override
                                 { return calcInputGradients(NeuronOCL); }
Â Â  virtual boolÂ Â Â Â Â Â updateInputWeights(CNeuronBaseOCL *NeuronOCL, CBufferFloat *Context) override
                                 { return updateInputWeights(NeuronOCL); }
Â Â  virtual boolÂ Â Â Â Â Â calcInputGradients(CNeuronBaseOCL *NeuronOCL) override;
Â Â  virtual boolÂ Â Â Â Â Â updateInputWeights(CNeuronBaseOCL *NeuronOCL) override;

public:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  CNeuronSceneSpecific(void) {};
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ~CNeuronSceneSpecific(void) {};
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
                          uint window, uint window_key, uint heads, uint heads_kv,
                          uint units_count, uint units_count_kv, uint layers, uint layers_to_one_kv,
                          ENUM_OPTIMIZATION optimization_type, uint batch);
Â Â  //---
Â Â  virtual intÂ Â Â Â Â Â  Type(void) overrideÂ Â constÂ Â  {Â Â return defNeuronSceneSpecific;Â Â  }
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Save(int const file_handle) override;
Â Â  virtual boolÂ Â Â Â Â Â Load(int const file_handle) override;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â WeightsUpdate(CNeuronBaseOCL *source, float tau) override;
Â Â  virtual voidÂ Â Â Â Â Â SetOpenCL(COpenCLMy *obj) override;
Â Â };
```

Here, it is important to highlight one fundamental difference between our new class and its parent. The parent class requires two sources of data to function correctly: the input data and a context. In our new class, however, the context is represented by learned scene-specific data derived from the training set. This data is learned through two internal layers: _cOne_ andÂ _cSceneSpecificKnowledge_. Essentially, this forms a two-layer MLP that takes a scalar input (1) and generates a tensor representing scene-specific knowledge. As you might expect, during inference, this tensor remains static. However, during training, the model "writes" the necessary information into it.

Following this logic, we exclude pointers to the external context from the methods of our new class.

All objects are declared statically, allowing us to leave the class constructor and destructor "empty". Object initialization is performed in the _Init_ method. In its parameters, we obtain the main constants of the object architecture. The functionality of the parameters used is similar to the relevant method of the parent class.

```
bool CNeuronSceneSpecific::Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
                                uint window, uint window_key, uint heads, uint heads_kv,
                                uint units_count, uint units_count_kv, uint layers, uint layers_to_one_kv,
                                ENUM_OPTIMIZATION optimization_type, uint batch)
Â Â {
Â Â  if(!CNeuronMLCrossAttentionMLKV::Init(numOutputs, myIndex, open_cl, window, window_key, heads, 16, heads_kv,
                              units_count, units_count_kv, layers, layers_to_one_kv, optimization_type, batch))
Â Â Â Â Â Â return false;
```

In the body of the method, we first call the initialization method of the parent class, passing along all the parameters received. This parent method handles parameter validation and the setup of inherited components.

Next, we initialize the previously mentioned scene-specific _MLP_.

Note that the first layer contains only a single constant input. The second layer generates a set of embedding vectors representing scene-specific knowledge. Each embedding vector is 16 elements in length. The number of embeddings is defined by method parameters and depends on the complexity of the environment being modeled.

```
Â Â  if(!cOne.Init(16 * units_count_kv, 0, OpenCL, 1, optimization, iBatch))
Â Â Â Â Â Â return false;
Â Â  CBufferFloat *out = cOne.getOutput();
Â Â  if(!out.BufferInit(1, 1) || !out.BufferWrite())
Â Â Â Â Â Â return false;
Â Â  if(!cSceneSpecificKnowledge.Init(0, 1, OpenCL, 16 * units_count_kv, optimization, iBatch))
Â Â Â Â Â Â return false;
//---
Â Â  return true;
Â Â }
```

Before concluding the method, we return a boolean value indicating the success or failure of the initialization to the calling function.

The initialization method of our new class is relatively short and concise. For good reason: the core functionality has already been implemented in the parent class. This pattern holds true not only for the initialization method. This also applies to the _feedForward_ method, whose parameters include a pointer to the source input data.

```
bool CNeuronSceneSpecific::feedForward(CNeuronBaseOCL *NeuronOCL)
Â Â {
Â Â  if(bTrain && !cSceneSpecificKnowledge.FeedForward(cOne.AsObject()))
Â Â Â Â Â Â return false;
```

In the body of the method, we first need to generate a matrix of learned context-dependent representations of the scene. But we perform this operation only in the model training process, when the resulting tensor is changed as we adjust the parameters of our _MLP_. During the model operation, these learned values are static and do not need to be recomputed. We simply reuse the pre-saved information.

Finally, we call the feedForward method of the parent class, passing in our scene-specific knowledge tensor as the context input.

```
Â Â  if(!CNeuronMLCrossAttentionMLKV::feedForward(NeuronOCL, cSceneSpecificKnowledge.getOutput()))
Â Â Â Â Â Â return false;
//---
Â Â  return true;
Â Â }
```

The backpropagation pass methods are constructed in a similar way. For the sake of conciseness and to keep the article from growing excessively long, I suggest studying them independently. The full implementation of the class and all its methods can be found in the attached materials.

#### 2.2 Constructing the _MSA_ Block

Now, we move on to constructing the Multi-Head Scene-Conditioned Attention (MSA) block. Naturally, we will inherit the core functionality from one of the previously implemented attention blocks. The structure of the new class _CNeuronMLMHSceneConditionAttention_ is presented below.

```
class CNeuronMLMHSceneConditionAttentionÂ Â :Â Â public CNeuronMLMHAttentionMLKV
Â Â {
protected:
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cSceneAgnostic;
Â Â  CLayerÂ Â Â Â Â Â Â Â Â Â Â Â cSceneSpecific;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â feedForward(CNeuronBaseOCL *NeuronOCL) override;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â calcInputGradients(CNeuronBaseOCL *prevLayer) override;
Â Â  virtual boolÂ Â Â Â Â Â updateInputWeights(CNeuronBaseOCL *NeuronOCL) override;

public:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  CNeuronMLMHSceneConditionAttention(void) {};
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ~CNeuronMLMHSceneConditionAttention(void) {};
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
                          uint window, uint window_key, uint heads, uint heads_kv,
                          uint units_count, uint layers, uint layers_to_one_kv,
                          ENUM_OPTIMIZATION optimization_type, uint batch) override;
Â Â  //---
Â Â  virtual intÂ Â Â Â Â Â  Type(void) overrideÂ Â constÂ Â  {Â Â return defNeuronMLMHSceneConditionAttention;Â Â  }
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Save(int const file_handle) override;
Â Â  virtual boolÂ Â Â Â Â Â Load(int const file_handle) override;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â WeightsUpdate(CNeuronBaseOCL *source, float tau) override;
Â Â  virtual voidÂ Â Â Â Â Â SetOpenCL(COpenCLMy *obj) override;
Â Â };
```

Within this structure, you'll notice the declaration of two new objects of type _CLayer_. One will store context-dependent representations of the scene, while the other holds general object-related information that is independent of the scene.

It is important to note that the presence of these two objects does not restrict the creation of deeper, nested neural layers for object identification. In this case, the _CLayer_ objects are used as dynamic arrays. While the number of internal neural layers is defined by the user during object initialization.

All internal objects are declared as static, allowing us to leave the constructor and destructor empty. As usual, the initialization of all internal and inherited objects is performed in the _Init_ method.

```
bool CNeuronMLMHSceneConditionAttention::Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
                                              uint window, uint window_key, uint heads, uint heads_kv,
                                              uint units_count, uint layers, uint layers_to_one_kv,
                                              ENUM_OPTIMIZATION optimization_type, uint batch)
Â Â {
Â Â  if(!CNeuronBaseOCL::Init(numOutputs, myIndex, open_cl, window * units_count, optimization_type, batch))
Â Â Â Â Â Â return false;
```

In the method parameters we receive the main constants that determine the architecture of the created object. And in the body of the method, we immediately call the relevant ancestor method. However, in this case we use the method not of the direct parent class, but of the base fully connected layer _CNeuronBaseOCL_. This is due to significant differences in the structure and size of inherited objects.

After the operations of the ancestor initialization method have successfully completed, we will save the architectural constants of our new class.

```
Â Â  iWindow = fmax(window, 1);
Â Â  iWindowKey = fmax(window_key, 1);
Â Â  iUnits = fmax(units_count, 1);
Â Â  iHeads = fmax(heads, 1);
Â Â  iLayers = fmax(layers, 1);
Â Â  iHeadsKV = fmax(heads_kv, 1);
Â Â  iLayersToOneKV = fmax(layers_to_one_kv, 1);
```

Then we calculate the dimensions of the internal objects.

```
Â Â  uint num_q = iWindowKey * iHeads * iUnits;Â Â Â Â Â Â Â Â Â Â Â Â Â Â  //Size of Q tensor
Â Â  uint num_kv = 2 * iWindowKey * iHeadsKV * iUnits;Â Â Â Â Â Â Â Â //Size of KV tensor
Â Â  uint q_weights = (iWindow * iHeads) * iWindowKey;Â Â Â Â Â Â Â Â //Size of weights' matrix of Q tenzor
Â Â  uint kv_weights = 2 * (iWindow * iHeadsKV) * iWindowKey; //Size of weights' matrix of KV tenzor
Â Â  uint scores = iUnits * iUnits * iHeads;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â //Size of Score tensor
Â Â  uint mh_out = iWindowKey * iHeads * iUnits;Â Â Â Â Â Â Â Â Â Â Â Â Â Â //Size of multi-heads self-attention
Â Â  uint out = iWindow * iUnits;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  //Size of out tensore
Â Â  uint w0 = (iWindowKey * iHeads + 1) * iWindow;Â Â Â Â Â Â Â Â Â Â  //Size W0 tensor
Â Â  uint ff_1 = 4 * (iWindow + 1) * iWindow;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  //Size of weights' matrix 1-st feed forward layer
Â Â  uint ff_2 = (4 * iWindow + 1) * iWindow;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  //Size of weights' matrix 2-nd feed forward layer
```

WE add 2 more local variables for temporary storage of pointers to neural layer objects.

```
Â Â  CNeuronBaseOCL *base = NULL;
Â Â  CNeuronSceneSpecific *ss = NULL;
```

This concludes the preparatory work. Next, we organize a loop with the number of iterations equal to the number of internal layers specified in the method parameters by the user. At each iteration of this loop, we create objects of one inner layer. Accordingly, after the full completion of the specified number of loop iterations, we create a complete set of objects necessary for the normal functioning of the required number of internal neural layers.

```
Â Â  for(uint i = 0; i < iLayers; i++)
Â Â Â Â  {
Â Â Â Â Â Â CBufferFloat *temp = NULL;
Â Â Â Â Â Â for(int d = 0; d < 2; d++)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  //--- Initilize Q tensor
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(num_q, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!QKV_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

In the body of the loop, we immediately create another nested loop of 2 iterations. In the body of the nested loop, we save data buffers to record the main flow data of the forward pass and the corresponding error gradients of the backpropagation pass. This 2-iteration loop allows us to create a mirror architecture for the feed-forward and backpropagation passes.

Here first we create a buffer to record the generated _Query_ entities. Then we create a buffer for recording the weight matrix for generating this entity.

```
Â Â Â Â Â Â Â Â  //--- Initilize Q weights
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(q_weights, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!QKV_Weights.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

Note that previously we always filled the weight matrix with random values that were adjusted during the model training process. This time, we have created a buffer with zero values. This is due to the implementation of the hypernetwork architecture, which will generate this matrix taking into account the analyzed scene.

We repeat similar operations to generate _Key_ and _Value_ entity data buffers. But here we also add the ability to use one tensor for several internal layers. Therefore, before creating data buffers, we check the feasibility of such operations.

```
Â Â Â Â Â Â Â Â  if(i % iLayersToOneKV == 0)
Â Â Â Â Â Â Â Â Â Â  {
Â Â Â Â Â Â Â Â Â Â Â Â //--- Initilize KV tensor
Â Â Â Â Â Â Â Â Â Â Â Â temp = new CBufferFloat();
Â Â Â Â Â Â Â Â Â Â Â Â if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â if(!temp.BufferInit(num_kv, 0))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â if(!KV_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â //--- Initilize KV weights
Â Â Â Â Â Â Â Â Â Â Â Â temp = new CBufferFloat();
Â Â Â Â Â Â Â Â Â Â Â Â if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â if(!temp.BufferInit(kv_weights, 0))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â if(!KV_Weights.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â  }
```

Next, we add a buffer to store the dependency coefficients.

```
Â Â Â Â Â Â Â Â  //--- Initialize scores
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(scores, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!S_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

And another buffer to store the results of multi-headed attention.

```
Â Â Â Â Â Â Â Â  //--- Initialize multi-heads attention out
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(mh_out, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!AO_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

As before, the results of multi-headed attention will scale to the size of the original data. We will write the result of this operation into the corresponding data buffer.

```
Â Â Â Â Â Â Â Â  //--- Initialize attention out
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(out, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!FF_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

Let;'s also add _FeedForward_ block operation buffers.

```
Â Â Â Â Â Â Â Â  //--- Initialize Feed Forward 1
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(4 * out, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!FF_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  //--- Initialize Feed Forward 2
Â Â Â Â Â Â Â Â  if(i == iLayers - 1)
Â Â Â Â Â Â Â Â Â Â  {
Â Â Â Â Â Â Â Â Â Â Â Â if(!FF_Tensors.Add(d == 0 ? Output : Gradient))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â Â Â Â Â Â Â continue;
Â Â Â Â Â Â Â Â Â Â  }
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(out, 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!FF_Tensors.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

As in the parent's corresponding method, we do not create new output buffers for the final internal layer of the FeedForward block. Instead, we point directly to the buffers inherited from the base fully connected layer. These buffers are used for inter-layer data exchange. And we write to them directly during execution, avoiding unnecessary data transfer between internal and external interfaces.

After initializing data buffers for the feed-forward and backpropagation data streams, we proceed to initialize the weight matrices. However, in our case, the generation of the _Query_, _Key_, and _Value_ entities, adapted to the current scene state, relies on two hypernetworks: one for scene-independent (prior) knowledge of objects and one for scene-dependent (contextual) knowledge. These hypernetworks also require initialization.

At this point, different implementation strategies are possible. As you know, _Key_ and _Value_, unlike _Query_, may not need to be generated at every internal layer. Therefore, we allocate them into a separate tensor. We could, theoretically, create separate hypernetworks for each corresponding weight matrix. However, that approach is suboptimal in terms of performance. Because it would increase the number of sequential operations. Instead, we opted to generate the required tensors in parallel using unified hypermodels, and then distribute the outputs into the appropriate data buffers.

However, _Key_ and _Value_ are not required on every internal layer. Therefore, we take that into account and, in such cases, simply utilize a model that returns a smaller result tensor.

Sounds logical? Greatâ€”letâ€™s move on to implementation. First, we divide the operation stream into two branches, depending on whether or not _Key-Value_ tensors need to be generated. The execution algorithm is identical for both streams. The only difference lies in the size of the resulting tensors.

```
Â Â Â Â Â Â if(i % iLayersToOneKV == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  //--- Initilize Scene-Specific layers
Â Â Â Â Â Â Â Â  ss = new CNeuronSceneSpecific();
Â Â Â Â Â Â Â Â  if(!ss)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!ss.Init((q_weights + kv_weights), cSceneSpecific.Total(), OpenCL, iWindow, iWindowKey,
                                                     4, 2, iUnits, 100, 2, 2, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSceneSpecific.Add(ss))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

First, we work with the context-dependent representation model. Here, we create and initialize a dynamic instance of the scene-specific knowledge module that we implemented earlier. A pointer to this newly created object is stored in the _cSceneSpecific_ array,which forms part of the context-conditioned model.

However, one key nuance should be noted here. The scene-specific knowledge module was built upon a cross-attention block, which takes as input the analyzed state of the scene. And it returns a tensor of the appropriate dimensions, enriched with context-dependent knowledge. The issue is that the size of the input tensor may not match the required dimensions of the target weight matrix. To resolve this, we introduce a fully connected scaling layer that adjusts the dimensions accordingly.

```
Â Â Â Â Â Â Â Â  base = new CNeuronBaseOCL();
Â Â Â Â Â Â Â Â  if(!base)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!base.Init(0, cSceneSpecific.Total(), OpenCL, (q_weights + kv_weights), optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  base.SetActivationFunction(TANH);
Â Â Â Â Â Â Â Â  if(!cSceneSpecific.Add(base))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

This scaling layer employs the hyperbolic tangent (tanh) as its activation function, which outputs values in the range \[-1, 1\]. As a result, the context-conditioned knowledge about the scene essentially acts as a flagging mechanism, indicating the likelihood or presence of certain objects within the analyzed scene.

For the scene-independent prior knowledge model, we use a two-layer MLP, similar in structure to the one described earlier for maintaining context-conditioned embeddings.

```
Â Â Â Â Â Â Â Â  //--- Initilize Scene-Agnostic layers
Â Â Â Â Â Â Â Â  base = new CNeuronBaseOCL();
Â Â Â Â Â Â Â Â  if(!base)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!base.Init((q_weights + kv_weights), cSceneAgnostic.Total(), OpenCL, 1, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  temp = base.getOutput();
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(1, 1) || !temp.BufferWrite())
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSceneAgnostic.Add(base))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  base = new CNeuronBaseOCL();
Â Â Â Â Â Â Â Â  if(!base)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!base.Init(0, cSceneAgnostic.Total(), OpenCL, (q_weights + kv_weights), optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSceneAgnostic.Add(base))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

If there is no need to generate a _Key-Value_ tensor, we create similar objects, but smaller in size.

```
Â Â Â Â Â Â else
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  //--- Initilize Scene-Specific layers
Â Â Â Â Â Â Â Â  ss = new CNeuronSceneSpecific();
Â Â Â Â Â Â Â Â  if(!ss)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!ss.Init(q_weights, cSceneSpecific.Total(), OpenCL, iWindow, iWindowKey,
                                           4, 2, iUnits, 100, 2, 2, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSceneSpecific.Add(ss))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  base = new CNeuronBaseOCL();
Â Â Â Â Â Â Â Â  if(!base)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!base.Init(0, cSceneSpecific.Total(), OpenCL, q_weights, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  base.SetActivationFunction(TANH);
Â Â Â Â Â Â Â Â  if(!cSceneSpecific.Add(base))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  //--- Initilize Scene-Agnostic layers
Â Â Â Â Â Â Â Â  base = new CNeuronBaseOCL();
Â Â Â Â Â Â Â Â  if(!base)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!base.Init(q_weights, cSceneAgnostic.Total(), OpenCL, 1, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  temp = base.getOutput();
Â Â Â Â Â Â Â Â  if(!temp.BufferInit(1, 1) || !temp.BufferWrite())
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSceneAgnostic.Add(base))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  base = new CNeuronBaseOCL();
Â Â Â Â Â Â Â Â  if(!base)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!base.Init(0, cSceneAgnostic.Total(), OpenCL, q_weights, optimization, iBatch))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!cSceneAgnostic.Add(base))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

For the multi-head attention result data scaling layer and the _FeedForward_ block, we use regular matrices of training parameters initialized with random parameters.

```
Â Â Â Â Â Â //--- Initilize Weights0
Â Â Â Â Â Â temp = new CBufferFloat();
Â Â Â Â Â Â if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!temp.Reserve(w0))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â float k = (float)(1 / sqrt(iWindow + 1));
Â Â Â Â Â Â for(uint w = 0; w < w0; w++)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(!temp.Add(GenerateWeight() * 2 * k - k))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!FF_Weights.Add(temp))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â //--- Initilize FF Weights
Â Â Â Â Â Â temp = new CBufferFloat();
Â Â Â Â Â Â if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!temp.Reserve(ff_1))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â for(uint w = 0; w < ff_1; w++)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(!temp.Add(GenerateWeight() * 2 * k - k))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!FF_Weights.Add(temp))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â //---
Â Â Â Â Â Â temp = new CBufferFloat();
Â Â Â Â Â Â if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!temp.Reserve(ff_2))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â k = (float)(1 / sqrt(4 * iWindow + 1));
Â Â Â Â Â Â for(uint w = 0; w < ff_2; w++)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(!temp.Add(GenerateWeight() * 2 * k - k))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(!FF_Weights.Add(temp))
Â Â Â Â Â Â Â Â  return false;
```

We also add moment buffers that we use in the process of optimizing the created matrices of training parameters. The number of moment buffers is determined by the specified parameter optimization method.

```
Â Â Â Â Â Â //---
Â Â Â Â Â Â for(int d = 0; d < (optimization == SGD ? 1 : 2); d++)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit((d == 0 || optimization == ADAM ? w0 : iWindow), 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!FF_Weights.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  //--- Initilize FF Weights
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit((d == 0 || optimization == ADAM ? ff_1 : 4 * iWindow), 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!FF_Weights.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  temp = new CBufferFloat();
Â Â Â Â Â Â Â Â  if(CheckPointer(temp) == POINTER_INVALID)
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferInit((d == 0 || optimization == ADAM ? ff_2 : iWindow), 0))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!temp.BufferCreate(OpenCL))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!FF_Weights.Add(temp))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â  }
```

After successfully initializing all the specified objects, we move on to the next iteration of the loop, in which we will create similar objects for the subsequent inner layer.

At the end of the initialization method, we initialize the auxiliary buffer for storing temporary data and return the logical result of the operations to the calling program.

```
Â Â  if(!Temp.BufferInit(MathMax((num_q + num_kv)*iWindow, out), 0))
Â Â Â Â Â Â return false;
Â Â  if(!Temp.BufferCreate(OpenCL))
Â Â Â Â Â Â return false;
//---
Â Â  SetOpenCL(OpenCL);
//---
Â Â  return true;
Â Â }
```

The complex structure and large number of objects make it difficult to understand the algorithm being constructed. Moreover, we need to carefully monitor the flow of information and data transfer between objects during the implementation of feed-forward and backpropagation pass methods. And we will start with the construction of the _feedForward_ method.

```
bool CNeuronMLMHSceneConditionAttention::feedForward(CNeuronBaseOCL *NeuronOCL)
Â Â {
Â Â  CNeuronBaseOCL *ss = NULL, *sa = NULL;
Â Â  CBufferFloat *q_weights = NULL, *kv_weights = NULL, *q = NULL, *kv = NULL;
```

In the method parameters, we receive a pointer to the source data object. However, we do not organize a check of the relevance of the received index. Because we do not plan to directly access the source data object at this stage. However, we will do a little preparatory work, during which we will create local variables for temporary storage of pointers to various objects. And then we'll create a loop to iterate through the internal layers of our block.

```
Â Â  for(uint i = 0; i < iLayers; i++)
Â Â Â Â  {
Â Â Â Â Â Â //--- Scene-Specific
Â Â Â Â Â Â ss = cSceneSpecific[i * 2];
Â Â Â Â Â Â if(!ss.FeedForward(NeuronOCL))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â ss = cSceneSpecific[i * 2 + 1];
Â Â Â Â Â Â if(!ss.FeedForward(cSceneSpecific[i * 2]))
Â Â Â Â Â Â Â Â  return false;
```

Inside the loop, we begin by generating the weight coefficients required to construct the Query, Key, and Value entities, using the previously defined hypernetworks.

First, based on the scene state description received from the calling program, we generate the matrix of context-dependent parameters. As described earlier, after enriching this tensor with context-specific knowledge, we rescale it to match the dimensions of the required parameter matrix.

At the same time, we generate the matrix of scene-independent parameters.

```
Â Â Â Â Â Â //--- Scene-Agnostic
Â Â Â Â Â Â sa = cSceneAgnostic[i * 2 + 1];
Â Â Â Â Â Â if(bTrain && !sa.FeedForward(cSceneAgnostic[i * 2]))
Â Â Â Â Â Â Â Â  return false;
```

Note that the scene-independent parameter matrix is only generated during model training. During operation, the matrix remains static. So there is no need to regenerate it for each feed-forward pass.

Next, we perform element-wise multiplication of the two matrices. The result is the final weight matrix, which we then distribute into the previously initialized data buffers. It's important to note that this is a single generated weight matrix, which is split into two parts. One part is used to create _Query_ entities. The second part is used for _Key_ and _Value_ entities. However, Key and Value are not necessarily formed on every layer. Therefore, we need to branch the operation flow depending on whether the _Key-Value_ tensor generation is required.

Before doing so, we carry out a bit of preliminary setup. Specifically, we transfer a pointer to the input data object of the current internal layer into a local variable.

```
Â Â Â Â Â Â CBufferFloat *inputs = (i == 0 ? NeuronOCL.getOutput() : FF_Tensors.At(6 * i - 4));
```

Here we store the pointer to the internal layer's input. This means that the pointer received from the external program is only passed into the first internal layer. For all subsequent layers, we use the outputs from the previous internal layer.

We also store in local variables the pointers to the data buffers of the weight parameters and the Query tensor, both of which are used regardless of which of the two processing paths we follow.

```
Â Â Â Â Â Â q_weights = QKV_Weights[i * 2];
Â Â Â Â Â Â q = QKV_Tensors[i * 2];
```

If it is necessary to form a _Key-Value_ tensor, we first perform element-wise multiplication of the two weight matrices formed above. And we write the result of the operation into the temporary data storage buffer.

```
Â Â Â Â Â Â if(i % iLayersToOneKV == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(IsStopped() || !ElementMult(ss.getOutput(), sa.getOutput(), GetPointer(Temp)))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
```

We save pointers to the weight and _Key-Value_ entity value buffers in local variables.

```
Â Â Â Â Â Â Â Â  kv_weights = KV_Weights[(i / iLayersToOneKV) * 2];
Â Â Â Â Â Â Â Â  kv = KV_Tensors[(i / iLayersToOneKV) * 2];
```

Then we distribute the common tensor of weight parameters across two data buffers.

```
Â Â Â Â Â Â Â Â  if(IsStopped() || !DeConcat(q_weights, kv_weights, GetPointer(Temp), iHeads, 2 * iHeadsKV,
                                                                              iWindow * iWindowKey))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(IsStopped() || !MatMul(inputs, kv_weights, kv, iUnits, iWindow, 2 * iHeadsKV * iWindowKey, 1))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

After that we will form a tensor of _Key-Value_ entities by matrix multiplication of the initial data tensor of the current internal layer by the obtained matrix of weight coefficients.

If there is no need to form a _Key-Value_ entity tensor, we only perform the operation of element-wise multiplication of two parameter matrices with the results written to the corresponding data buffer. Bevause in this case, our hypernetworks form only a weight matrix of the _Query_ entity.

```
Â Â Â Â Â Â else
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(IsStopped() || !ElementMult(ss.getOutput(), sa.getOutput(), q_weights))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

The formation of the Query entity value tensor is performed in any case. Therefore, we implement this operation in the general flow.

```
Â Â Â Â Â Â if(IsStopped() || !MatMul(inputs, q_weights, q, iUnits, iWindow, iHeads * iWindowKey, 1))
Â Â Â Â Â Â Â Â  return false;
```

At this stage, the implementation of hypernetworks into the attention algorithm is completed. Then follows the mechanism that is already familiar to us: _Self-Attention_. First, we determine the results of multi-headed attention.

```
Â Â Â Â Â Â //--- Score calculation and Multi-heads attention calculation
Â Â Â Â Â Â CBufferFloat *temp = S_Tensors[i * 2];
Â Â Â Â Â Â CBufferFloat *out = AO_Tensors[i * 2];
Â Â Â Â Â Â if(IsStopped() || !AttentionOut(q, kv, temp, out))
Â Â Â Â Â Â Â Â  return false;
```

Then we reduce the dimensionality of the resulting output tensor.

```
Â Â Â Â Â Â //--- Attention out calculation
Â Â Â Â Â Â temp = FF_Tensors[i * 6];
Â Â Â Â Â Â if(IsStopped() ||
         !ConvolutionForward(FF_Weights[i * (optimization == SGD ? 6 : 9)], out, temp,
                                                   iWindowKey * iHeads, iWindow, None))
Â Â Â Â Â Â Â Â  return false;
```

After that we sum up the results of the _Self-Attention_ block with the original data and normalize the resulting tensor.

```
Â Â Â Â Â Â //--- Sum and normilize attention
Â Â Â Â Â Â if(IsStopped() || !SumAndNormilize(temp, inputs, temp, iWindow, true))
Â Â Â Â Â Â Â Â  return false;
```

Next the data passes through the _FeedForward_ block.

```
Â Â Â Â Â Â //--- Feed Forward
Â Â Â Â Â Â inputs = temp;
Â Â Â Â Â Â temp = FF_Tensors[i * 6 + 1];
Â Â Â Â Â Â if(IsStopped() ||
         !ConvolutionForward(FF_Weights[i * (optimization == SGD ? 6 : 9) + 1], inputs, temp,
                                                                 iWindow, 4 * iWindow, LReLU))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â out = FF_Tensors[i * 6 + 2];
Â Â Â Â Â Â if(IsStopped() ||
         !ConvolutionForward(FF_Weights[i * (optimization == SGD ? 6 : 9) + 2], temp, out,
                                                           4 * iWindow, iWindow, activation))
Â Â Â Â Â Â Â Â  return false;
```

Then we sum and normalize the data.

```
Â Â Â Â Â Â //--- Sum and normilize out
Â Â Â Â Â Â if(IsStopped() || !SumAndNormilize(out, inputs, out, iWindow, true))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â  }
//--- result
Â Â  return true;
Â Â }
```

We repeat the operations for all internal layers. After we complete all loop iterations, the logical result of executing the method operations is returned to the calling program .

I hope at this stage, you understand how our class's algorithm works. But there is another nuance related to the distribution of the error gradient during the backpropagation pass - its algorithm is implemented in the _calcInputGradients_ method.

```
bool CNeuronMLMHSceneConditionAttention::calcInputGradients(CNeuronBaseOCL *prevLayer)
Â Â {
Â Â  if(CheckPointer(prevLayer) == POINTER_INVALID)
Â Â Â Â Â Â return false;
```

In the method parameters, as usual, we receive a pointer to the object of the previous layer, to which we must propagate the error gradient in accordance with the influence of the original data on the final result. In the body of the method, we immediately check the relevance of the received pointer. After that, we create several local variables for temporary storage of pointers to objects.

```
Â Â  CBufferFloat *out_grad = Gradient;
Â Â  CBufferFloat *kv_g = KV_Tensors[KV_Tensors.Total() - 1];
Â Â  CNeuronBaseOCL *ss = NULL, *sa = NULL;
```

Then we organize a reverse loop through the internal layers.

```
Â Â  for(int i = int(iLayers - 1); (i >= 0 && !IsStopped()); i--)
Â Â Â Â  {
Â Â Â Â Â Â if(i == int(iLayers - 1) || (i + 1) % iLayersToOneKV == 0)
Â Â Â Â Â Â Â Â  kv_g = KV_Tensors[(i / iLayersToOneKV) * 2 + 1];
```

As you are aware, the gradient backpropagation algorithm mirrors the feed-forward pass, with all operations executed in reverse order. For this reason, we structured a reverse iteration loop over the internal layers of our block.

Let me remind you that the second half of the forward pass operations directly replicated the corresponding logic in the parent class. Consequently, we begin the first half of our gradient backpropagation method by reusing the corresponding method from the parent class.

We start by backpropagating the error gradient through the _FeedForward_ block.

```
Â Â Â Â Â Â //--- Passing gradient through feed forward layers
Â Â Â Â Â Â if(IsStopped() ||
        !ConvolutionInputGradients(FF_Weights[i * (optimization == SGD ? 6 : 9) + 2], out_grad,
                       FF_Tensors[i * 6 + 1], FF_Tensors[i * 6 + 4], 4 * iWindow, iWindow, None))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â CBufferFloat *temp = FF_Tensors[i * 6 + 3];
Â Â Â Â Â Â if(IsStopped() ||
        !ConvolutionInputGradients(FF_Weights[i * (optimization == SGD ? 6 : 9) + 1],
                     FF_Tensors[i * 6 + 4], FF_Tensors[i * 6], temp, iWindow, 4 * iWindow, LReLU))
Â Â Â Â Â Â Â Â  return false;
```

After that we sum the error gradient from the two information streams.

```
Â Â Â Â Â Â //--- Sum and normilize gradients
Â Â Â Â Â Â if(IsStopped() || !SumAndNormilize(out_grad, temp, temp, iWindow, false))
Â Â Â Â Â Â Â Â  return false;
```

And then we propagate the error gradient through the _Multi-Head Self-Attention_ block.

```
Â Â Â Â Â Â //--- Sum and normilize gradients
Â Â Â Â Â Â if(IsStopped() || !SumAndNormilize(out_grad, temp, temp, iWindow, false))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â out_grad = temp;
Â Â Â Â Â Â //--- Split gradient to multi-heads
Â Â Â Â Â Â if(IsStopped() ||
        !ConvolutionInputGradients(FF_Weights[i * (optimization == SGD ? 6 : 9)], out_grad,
                AO_Tensors[i * 2], AO_Tensors[i * 2 + 1], iWindowKey * iHeads, iWindow, None))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â //--- Passing gradient to query, key and value
Â Â Â Â Â Â sa = cSceneAgnostic[i * 2 + 1];
Â Â Â Â Â Â ss = cSceneSpecific[i * 2 + 1];
Â Â Â Â Â Â if(i == int(iLayers - 1) || (i + 1) % iLayersToOneKV == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(IsStopped() ||
         !AttentionInsideGradients(QKV_Tensors[i * 2], QKV_Tensors[i * 2 + 1],
                 KV_Tensors[(i / iLayersToOneKV) * 2], kv_g, S_Tensors[i * 2], AO_Tensors[i * 2 + 1]))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â else
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(IsStopped() ||
           !AttentionInsideGradients(QKV_Tensors[i * 2], QKV_Tensors[i * 2 + 1],
             KV_Tensors[i / iLayersToOneKV * 2], GetPointer(Temp), S_Tensors[i * 2], AO_Tensors[i * 2 + 1]))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(IsStopped() ||
           !SumAndNormilize(kv_g, GetPointer(Temp), kv_g, iWindowKey, false, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
```

Here, special attention should be paid to how the error gradient is propagated to the _Key-Value_ tensor. The nuance lies in collecting error gradients from all internal layers influenced by a given tensor. A more detailed explanation of this algorithm is provided in the [article](https://www.mql5.com/en/articles/15117#para32) dedicated to the parent class.

We then proceed to backpropagate the error gradient to the level of the input data along the main information flow.

```
Â Â Â Â Â Â CBufferFloat *inp = NULL;
Â Â Â Â Â Â if(i == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  inp = prevLayer.getOutput();
Â Â Â Â Â Â Â Â  temp = prevLayer.getGradient();
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â else
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  temp = FF_Tensors.At(i * 6 - 1);
Â Â Â Â Â Â Â Â  inp = FF_Tensors.At(i * 6 - 4);
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â if(IsStopped() ||
        !MatMulGrad(inp, temp, QKV_Weights[i * 2], QKV_Weights[i * 2 + 1], QKV_Tensors[i * 2 + 1],
                                                          iUnits, iWindow, iHeads * iWindowKey, 1))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â //--- Sum and normilize gradients
Â Â Â Â Â Â if(IsStopped() || !SumAndNormilize(out_grad, temp, temp, iWindow, false, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â  return false;
```

We also propagate the error gradient to the hypernetworks in accordance with their influence on the overall result of the model.

```
Â Â Â Â Â Â //---
Â Â Â Â Â Â if((i % iLayersToOneKV) == 0)
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(IsStopped() ||
           !MatMulGrad(inp, GetPointer(Temp), KV_Weights[i / iLayersToOneKV * 2],
                       KV_Weights[i / iLayersToOneKV * 2 + 1], KV_Tensors[i / iLayersToOneKV * 2 + 1],
                                                        iUnits, iWindow, 2 * iHeadsKV * iWindowKey, 1))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(IsStopped() || !SumAndNormilize(GetPointer(Temp), temp, temp, iWindow, false, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!Concat(QKV_Weights[i * 2 + 1], KV_Weights[i / iLayersToOneKV * 2 + 1], ss.getGradient(),
                                                           iHeads, 2 * iHeadsKV, iWindow * iWindowKey))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â  if(!ElementMultGrad(ss.getOutput(), ss.getGradient(), sa.getOutput(), sa.getGradient(),
                                                   ss.getGradient(), ss.Activation(), sa.Activation()))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â else
Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â  if(!ElementMultGrad(ss.getOutput(), ss.getGradient(), sa.getOutput(), sa.getGradient(),
                                              QKV_Weights[i * 2 + 1], ss.Activation(), sa.Activation()))
Â Â Â Â Â Â Â Â Â Â Â Â return false;
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â if(i > 0)
Â Â Â Â Â Â Â Â  out_grad = temp;
Â Â Â Â  }
```

After this, we move to the next iteration of our loop through internal layers.

It is important to note that in the main operation loop, we only propagated the error gradient down to the hypernetworks, but we did not propagate it through them. This introduces a couple of nuances. First, our scene-independent hypernetwork consists of only two layers. The first is static, always outputting a constant value of 1. And the second contains trainable parameters and returns the actual result. In the main operations stream, we pass the error gradient only to the second layer. Propagating the gradient to the static first layer would be meaningless. Of course, this is a specific case. If the hypermodel had more layers, we would need to implement gradient backpropagation logic for all layers containing trainable parameters.

The second nuance pertains to the scene-dependent hypernetwork. In this implementation, all parameters are generated based on the scene description received from the calling program. As a result, the entire error gradient must be propagated to that level. To maintain the integrity of the main data flow, we chose to handle gradient propagation for this model in a separate loop. Again, this is a specific implementation choice. If we were to derive the scene description from a different source (e.g., the output of a preceding internal layer), we would need to propagate the gradient accordingly.

Let's return to the algorithm of our gradient backpropagation method. After completing the reverse iteration over internal layers, we store in the gradient buffer of the previous layer the results representing how the input data influenced the model's output through the main data flow. We must now add the contribution from the hypernetworks to this buffer. To do this, we first store a pointer to the gradient buffer of the previous layer in a local variable. Then, we temporarily assign a pointer to our auxiliary data buffer to the current layer object.

```
Â Â  CBufferFloat *inp_grad = prevLayer.getGradient();
Â Â  if(!prevLayer.SetGradient(GetPointer(Temp), false))
Â Â Â Â Â Â return false;
```

Now we can propagate the error gradient down to the previous layer without fear of losing previously saved data. We create a loop iterating over the objects of our context-dependent hypernetwork; in its body we propagate the error gradient to the level of the source data layer. At each iteration, we add the current result to the previously accumulated gradient.

```
Â Â  for(int i = int(iLayers - 2); (i >= 0 && !IsStopped()); i -= 2)
Â Â Â Â  {
Â Â Â Â Â Â ss = cSceneSpecific[i];
Â Â Â Â Â Â if(IsStopped() || !ss.calcHiddenGradients(cSceneSpecific[i + 1]))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(IsStopped() || !prevLayer.calcHiddenGradients(ss, NULL))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â Â Â if(IsStopped() ||
        !SumAndNormilize(prevLayer.getGradient(), inp_grad, inp_grad, iWindow, false, 0, 0, 0, 1))
Â Â Â Â Â Â Â Â  return false;
Â Â Â Â  }
```

After the successful execution of the loop operations, we return to the object of the previous layer a pointer to its buffer with the already accumulated error gradient from all information flows.

```
Â Â  if(!prevLayer.SetGradient(inp_grad, false))
Â Â Â Â Â Â return false;
//---
Â Â  return true;
Â Â }
```

The error gradient is distributed in full and we return to the calling program the logical result of executing the operations of our error gradient distribution method. I suggest you familiarize yourself with the method of updating the model parameters independently. You can find the full code of this class and all its methods in the attachment.

#### 2.3 Constructing the Complete _HyperDet3D_ Algorithm

Earlier, we built the individual components of the _HyperDet3D_ algorithm. Now it's time to bring everything together into a single, cohesive structure. And while this may seem relatively straightforward, there are a few important nuances worth highlighting.

For this experiment, I chose to base the integration on the [Pointformer](https://www.mql5.com/en/articles/15820#para3) architecture discussed in the previous article. The key modification here is the replacement of the global attention block with our newly constructed _MSA_ module. This operation is quite simple. Especially since we've left all method parameters unchanged, including the class initialization method. However, there's a catch: all objects within the original _CNeuronPointFormer_ class were declared as static. As a result, we cannot inherit from the class and modify the types of its internal objects directly. Therefore, we create a duplicate of the class, in which we adjust the types of the necessary internal objects to incorporate the new functionality. The structure of the new class is shown below.

```
class CNeuronHyperDetÂ Â  :Â Â public CNeuronPointNet2OCL
Â Â {
protected:
Â Â  CNeuronMLMHSparseAttentionÂ Â Â Â caLocalAttention[2];
Â Â  CNeuronMLCrossAttentionMLKVÂ Â  caLocalGlobalAttention[2];
Â Â  CNeuronMLMHSceneConditionAttentionÂ Â caGlobalAttention[2];
Â Â  CNeuronLearnabledPEÂ Â Â Â Â Â Â Â Â Â  caLocalPE[2];
Â Â  CNeuronLearnabledPEÂ Â Â Â Â Â Â Â Â Â  caGlobalPE[2];
Â Â  CNeuronBaseOCLÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cConcatenate;
Â Â  CNeuronConvOCLÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cScale;
Â Â  //---
Â Â  CBufferFloatÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â *cbTemp;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â feedForward(CNeuronBaseOCL *NeuronOCL) override ;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â calcInputGradients(CNeuronBaseOCL *NeuronOCL) override;
Â Â  virtual boolÂ Â Â Â Â Â updateInputWeights(CNeuronBaseOCL *NeuronOCL) override;

public:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  CNeuronHyperDet(void) {};
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ~CNeuronHyperDet(void) { delete cbTemp; }
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Init(uint numOutputs, uint myIndex, COpenCLMy *open_cl,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â uint window, uint units_count, uint output, bool use_tnets,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ENUM_OPTIMIZATION optimization_type, uint batch) override;
Â Â  //---
Â Â  virtual intÂ Â Â Â Â Â  Type(void) overrideÂ Â  constÂ Â  {Â Â return defNeuronHyperDet; }
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â Save(int const file_handle) override;
Â Â  virtual boolÂ Â Â Â Â Â Load(int const file_handle) override;
Â Â  //---
Â Â  virtual boolÂ Â Â Â Â Â WeightsUpdate(CNeuronBaseOCL *source, float tau) override;
Â Â  virtual voidÂ Â Â Â Â Â SetOpenCL(COpenCLMy *obj) override;
Â Â };
```

We will not delve into the methods of this class, as all of them were created by directly copying the corresponding methods from the _[CNeuronPointFormer](https://www.mql5.com/en/articles/15820#para3)_ class.

The model architecture, along with all interaction and training scripts, were also borrowed from the previous article. Therefore, we will not discuss them now. The complete source code for all programs used in the preparation of this article is available in the attachment.

### 3\. Testing

We have conducted extensive work to implement our own interpretation of the approaches proposed by the authors of the _HyperDet3D_ method. Now it's time to move on to the final part of our article. Here, we train and test the models that incorporate the described techniques.

As always, to train the models we use real historical data of the _EURUSD_ instrument, with the _H1_ timeframe, for the whole of 2023. All indicator parameters were set to their default values. The training process itself follows the exact algorithm outlined in the previous [article](https://www.mql5.com/en/articles/15820#para4). So we will focus only on the results of testing the trained Actor policy, which are presented below.

![](https://c.mql5.com/2/135/2394476734506__1.png)![](https://c.mql5.com/2/135/1732613320642__1.png)

The trained model was tested on historical data from January 2024, which was not part of the training dataset. During this period, the model executed 41 trades, with 56% of them closing in profit. Notably, the largest winning trade was 2.4 times larger than the largest loss, and the average profitable trade exceeded the average losing trade by 67%. These outcomes resulted in a profit factor of 2.14 and a Sharpe ratio of 20.65.

Overall, the model achieved a 1% profit during the test period, while the maximum drawdown on equity did not exceed 0.34%. The balance drawdown was even lower. The equity curve shows a steady balance increase, and the account exposure remained within 1â€“2%.

The general impression of the results is positive. The model shows promise. However, the short testing period and limited number of trades prevent us from making conclusions about the model's long-term stability. Before it can be used in live trading, further training on a larger historical dataset and more comprehensive testing will be required.

### Conclusion

In this article, we explored the _HyperDet3D_ method, which integrates scene-conditioned hypernetworks into a _Transformer_ architecture to embed prior knowledge. This allows the model to adapt effectively to different scenes in object detection tasks by dynamically tuning detector parameters based on scene information. This makes the system more universal and powerful.

In the practical section, we implemented our interpretation of the proposed concepts using MQL5, integrating them into our custom model architecture. The test results demonstrate the potential of the model. However, further work is needed before it can be applied in real-world financial markets.

**References**

- [HyperDet3D: Learning a Scene-conditioned 3D Object Detector](https://www.mql5.com/go?link=https://arxiv.org/abs/2204.05599 "HyperDet3D: Learning a Scene-conditioned 3D Object Detector")
- [Other articles from this series](https://www.mql5.com/en/search#!keyword=neural%20networks&author=DNG&method=2)

**Programs used in the article**

| # | Issued to | Type | Description |
| --- | --- | --- | --- |
| 1 | Research.mq5 | Expert Advisor | EA for collecting examples |
| 2 | ResearchRealORL.mq5 | Expert Advisor | EA for collecting examples using the Real-ORL method |
| 3 | Study.mq5 | Expert Advisor | Model training EA |
| 4 | Test.mq5 | Expert Advisor | Model testing EA |
| 5 | Trajectory.mqh | Class library | System state description structure |
| 6 | NeuroNet.mqh | Class library | A library of classes for creating a neural network |
| 7 | NeuroNet.cl | Library | OpenCL program code library |

Translated from Russian by MetaQuotes Ltd.

Original article: [https://www.mql5.com/ru/articles/15859](https://www.mql5.com/ru/articles/15859)

**Attached files** \|


[Download ZIP](https://www.mql5.com/en/articles/download/15859.zip "Download all attachments in the single ZIP archive")

[MQL5.zip](https://www.mql5.com/en/articles/download/15859/mql5.zip "Download MQL5.zip")(1797.86 KB)

**Warning:** All rights to these materials are reserved by MetaQuotes Ltd. Copying or reprinting of these materials in whole or in part is prohibited.

This article was written by a user of the site and reflects their personal views. MetaQuotes Ltd is not responsible for the accuracy of the information presented, nor for any consequences resulting from the use of the solutions, strategies or recommendations described.

#### Other articles by this author

- [Neural Networks in Trading: Hybrid Graph Sequence Models (GSM++)](https://www.mql5.com/en/articles/17279)
- [Neural Networks in Trading: Two-Dimensional Connection Space Models (Final Part)](https://www.mql5.com/en/articles/17241)
- [Neural Networks in Trading: Two-Dimensional Connection Space Models (Chimera)](https://www.mql5.com/en/articles/17210)
- [Neural Networks in Trading: Multi-Task Learning Based on the ResNeXt Model (Final Part)](https://www.mql5.com/en/articles/17157)
- [Neural Networks in Trading: Multi-Task Learning Based on the ResNeXt Model](https://www.mql5.com/en/articles/17142)
- [Neural Networks in Trading: Hierarchical Dual-Tower Transformer (Final Part)](https://www.mql5.com/en/articles/17104)
- [Neural Networks in Trading: Hierarchical Dual-Tower Transformer (Hidformer)](https://www.mql5.com/en/articles/17069)

**[Go to discussion](https://www.mql5.com/en/forum/484982)**

![Creating a Trading Administrator Panel in MQL5 (Part X): External resource-based interface](https://c.mql5.com/2/135/Creating_a_Trading_Administrator_Panel_in_MQL5_Part_X__LOGO.png)[Creating a Trading Administrator Panel in MQL5 (Part X): External resource-based interface](https://www.mql5.com/en/articles/17780)

Today, we are harnessing the capabilities of MQL5 to utilize external resourcesâ€”such as images in the BMP formatâ€”to create a uniquely styled home interface for the Trading Administrator Panel. The strategy demonstrated here is particularly useful when packaging multiple resources, including images, sounds, and more, for streamlined distribution. Join us in this discussion as we explore how these features are implemented to deliver a modern and visually appealing interface for our New\_Admin\_Panel EA.

![Developing a Replay System (Part 65): Playing the service (VI)](https://c.mql5.com/2/93/Desenvolvendo_um_sistema_de_Replay_Parte_65__LOGO.png)[Developing a Replay System (Part 65): Playing the service (VI)](https://www.mql5.com/en/articles/12265)

In this article, we will look at how to implement and solve the mouse pointer issue when using it in conjunction with a replay/simulation application. The content presented here is intended solely for educational purposes. Under no circumstances should the application be viewed for any purpose other than to learn and master the concepts presented.

![Building a Custom Market Regime Detection System in MQL5 (Part 1): Indicator](https://c.mql5.com/2/135/Building_a_Custom_Market_Regime_Detection_System_in_MQL5_Part_1.png)[Building a Custom Market Regime Detection System in MQL5 (Part 1): Indicator](https://www.mql5.com/en/articles/17737)

This article details creating an MQL5 Market Regime Detection System using statistical methods like autocorrelation and volatility. It provides code for classes to classify trending, ranging, and volatile conditions and a custom indicator.

![Reimagining Classic Strategies (Part 14): High Probability Setups](https://c.mql5.com/2/135/Reimagining_Classic_Strategies_Part_14___LOGO.png)[Reimagining Classic Strategies (Part 14): High Probability Setups](https://www.mql5.com/en/articles/17756)

High probability Setups are well known in our trading community, but regrettably they are not well-defined. In this article, we will aim to find an empirical and algorithmic way of defining exactly what is a high probability setup, identifying and exploiting them. By using Gradient Boosting Trees, we demonstrated how the reader can improve the performance of an arbitrary trading strategy and better communicate the exact job to be done to our computer in a more meaningful and explicit manner.

[![](https://www.mql5.com/ff/sh/ub4fqgrk4rkv8gz9z2/01.png)![](https://www.mql5.com/ff/sh/ub4fqgrk4rkv8gz9z2/02.png)Explore your trading for freeUpdated statistics in MetaTrader 5 will help you to thoroughly evaluate results and reduce risksLearn more](https://www.mql5.com/ff/go?link=https://www.mql5.com/en/forum/454106&a=bkbqgaxtrafeuegfvjisjjwjohagrvnr&s=25c5856d7857fc6b6db7cffb15ae4ce40fd19d1ab594d8a900ad65673d9ffa0e&uid=&ref=https://www.mql5.com/en/articles/15859&id=wdausxxqrpvhekbwjrjlhqjghyhesrqqau&fz_uniq=5069612210650548189)

This website uses cookies. Learn more about our [Cookies Policy](https://www.mql5.com/en/about/cookies).

![close](https://c.mql5.com/i/close.png)

![MQL5 - Language of trade strategies built-in the MetaTrader 5 client terminal](https://c.mql5.com/i/registerlandings/logo-2.png)

You are missing trading opportunities:

- Free trading apps
- Over 8,000 signals for copying
- Economic news for exploring financial markets

RegistrationLog in

latin characters without spaces

a password will be sent to this email

An error occurred


- [Log in With Google](https://www.mql5.com/en/auth_oauth2?provider=Google&amp;return=popup&amp;reg=1)

You agree to [website policy](https://www.mql5.com/en/about/privacy) and [terms of use](https://www.mql5.com/en/about/terms)

If you do not have an account, please [register](https://www.mql5.com/en/auth_register)

Allow the use of cookies to log in to the MQL5.com website.

Please enable the necessary setting in your browser, otherwise you will not be able to log in.

[Forgot your login/password?](https://www.mql5.com/en/auth_forgotten?return=popup)

- [Log in With Google](https://www.mql5.com/en/auth_oauth2?provider=Google&amp;return=popup)