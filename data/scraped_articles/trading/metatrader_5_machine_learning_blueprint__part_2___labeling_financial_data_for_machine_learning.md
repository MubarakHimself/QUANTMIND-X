---
title: MetaTrader 5 Machine Learning Blueprint (Part 2): Labeling Financial Data for Machine Learning
url: https://www.mql5.com/en/articles/18864
categories: Trading, Machine Learning
relevance_score: 6
scraped_at: 2026-01-22T17:55:16.488365
---

[![](https://www.mql5.com/ff/sh/7h2yc16rtqsn2m6kz2/c0d1e95edf776bf88908b398733d0997.jpg)\\
MQL5 Channels - Market analysis\\
\\
Dozens of channels, thousands of subscribers and daily updates. Learn more about trading.\\
\\
Download](https://www.mql5.com/ff/go?link=https://www.metatrader5.com/en/news/2270%3Futm_source=www.mql5.com%26utm_medium=display%26utm_term=messenger.for.traders%26utm_content=download.app%26utm_campaign=0524.mql5.channels&a=glufvbpblsoxonicqfngsyuzwfebnilr&s=103cc3ab372a16872ca1698fc86368ffe3b3eaa21b59b4006d5c6c10f48ad545&uid=&ref=https://www.mql5.com/en/articles/18864&id=bfogggabsofabcpxuzmgaibarmaxasdrj&fz_uniq=5049483154928020562)

MetaTrader 5 / Trading


### Table of Contents

- [Introduction](https://www.mql5.com/en/articles/18864/196654/editintroduction)
- [Overview of Labeling Methods](https://www.mql5.com/en/articles/18864/196654/editoverview-of-labeling-methods)
- [Implementation: Triple-Barrier Labeling Method](https://www.mql5.com/en/articles/18864/196654/editimplementation-triple-barrier-labeling-method)
- [Meta-Labeling Reports: Bollinger Band Strategy](https://www.mql5.com/en/articles/18864/196654/editimplementation-meta-labeling-reports)
- [Conclusion](https://www.mql5.com/en/articles/18864/196654/editconclusion)

### Introduction

Picture this: You're training to become an elite sniper. Would you rather practice shooting at perfect circles on a paper target, or train with human-silhouette targets that mimic real combat scenarios? The answer is obviousâ€”you need targets that reflect the reality you'll face.

The same principle applies to machine learning in finance. Most academic research uses what's called "fixed-time horizon labeling", the equivalent of shooting at those perfect circles. This approach asks a simple question: _"Will the price be higher or lower in exactly X days?"_ But here's the problem: real traders don't just care about where the price ends up. They care about the _journey,_ that beingwhen their stop-loss gets hit, when they should take profits, and how the price moves along the way.

Welcome back to our MetaTrader 5 Machine Learning Blueprint series. In [Part 1](https://www.mql5.com/en/articles/17520), we solved the critical "timestamp trap" that silently destroys most trading algorithms. Now we're tackling an equally important challenge: how to create labels that actually reflect how you trade in the real world.

Think of it this way: if you're building a model to predict whether someone will have a heart attack, you wouldn't just look at whether they're alive or dead in exactly 365 days. You'd want to know about warning signs, early interventions, and the sequence of events that are relevant for medical decisions. Financial markets work the same way.

This article assumes you already know your way around Python and have a basic grasp of machine learning concepts. We'll be diving deep into practical code and real-world applications that you can implement immediately.

**Recap of Part 1: Data Leakage and Timestamp Fixes**

In the first installment in the series, we addressed a critical but often overlooked issue that can silently undermine machine learning models in financial markets: the " _timestamp trap_" and data leakage problems inherent in the MetaTrader 5 default data structure. We laid the critical groundwork by addressing data integrity,Â emphasizing the necessity of constructing clean, unbiased bars from raw tick data. This foundation is non-negotiable for developing reliable financial machine learning models. If you haven't reviewed [Part 1](https://www.mql5.com/en/articles/17520), we strongly recommend doing so before proceeding.

The foundation laid there ensures:

1. Data Integrity:Â All timestamps reflect when information was actually available.
2. Statistical Soundness: Activity-driven bars provide better statistical properties for ML models.
3. Real-world Alignment: Bar construction matches actual market information flow.

With these fundamental data quality issues resolved, we now have clean, unbiased datasets ready for the next critical step in our machine learning pipelineâ€”labeling techniques that truly capture market dynamics.

### Overview of Labeling Methods

Most machine learning models in finance fail for a surprisingly mundane reasonâ€”not because of bad algorithms or insufficient computing power, but because of terrible labels. When I started building trading models, I spent months optimizing features and trying different neural network architectures, only to discover that my labeling scheme was fundamentally flawed. I was essentially teaching my model to hit a bullseye when I really needed it to hit a moving target in a hurricane.

Financial markets are relentlessly noisy. Every tick contains some mix of genuine information and random chaos, and our job is to extract the signal while acknowledging that most price movements are just market participants changing their minds or algorithms reacting to microsecond-level fluctuations. Good labeling doesn't just tell us what happenedâ€”it helps us understand what matters for actual trading decisions.

The simplest approach, and where most people start, is fixed-time horizon labeling. You pick a periodâ€”say five daysâ€”and ask whether the price will be higher or lower at the end of that period. If Apple closes at $150 on Monday and hits $155 by Friday, Monday gets labeled as a "buy" signal. It's clean, intuitive, and fundamentally wrong for how real trading works. When was the last time you bought a stock thinking, "I'll check back in exactly five days no matter what happens"? If the stock crashes 20% on Tuesday, you're not waiting until Friday to reassess. If it jumps 15% on Wednesday, you might take profits immediately.

This brings us to the triple-barrier method, which changed how serious practitioners think about financial ML. Instead of arbitrary time horizons, you set up three barriers around each potential trade, just like a professional trader would. There's your profit targetâ€”maybe 5% above your entry price. There's your stop lossâ€”perhaps 3% below where you bought. And there's a time limit because you don't want to hold losing positions forever. Your label depends on which barrier gets hit first. Suddenly, your model isn't learning abstract price movements; it's learning whether specific trade setups are likely to hit profit targets before hitting stop losses.

The beauty of this approach is that it mirrors actual trading psychology. Real traders don't care if a stock eventually goes up if it first drops enough to trigger their risk management rules. The path matters as much as the destination, and the triple-barrier method captures this path dependency naturally. You can even make the barriers dynamicâ€”wider stops during volatile periods, tighter ones when markets are calm.

For markets where trends vary dramatically in length, trend-scanning methods offer an elegant solution. Instead of imposing a fixed timeframe, these algorithms test multiple forward-looking periods and identify the most statistically significant trend. Maybe the 5-day trend is weak, the 10-day trend is strong, and the 15-day trend is moderate. The method picks the strongest signal and labels accordingly. It's like having the algorithm itself determine the optimal prediction horizon for each market condition.

Then there's meta-labeling, which tackles an entirely different problem. Instead of predicting market direction, it asks: "When should I trust my other predictions?" Imagine you already have a trading strategy generating buy and sell signals. Meta-labeling builds a second model that evaluates whether each signal is likely to be profitable. Your main strategy might say "buy," but the meta-model considers additional factorsâ€”recent performance, market volatility, time since the last major news eventâ€”and outputs a confidence score. High confidence means you size the position aggressively. Low confidence means you pass or bet small.

This approach recognizes a crucial insight: knowing when you're likely to be right is often more valuable than trying to be right more often. It's the difference between being a good forecaster and being a profitable trader. Many strategies fail not because their directional predictions are poor, but because they bet the same amount on high-confidence and low-confidence signals.

Each labeling method teaches your model different lessons. Fixed-horizon methods focus on pure price direction. Triple-barrier methods incorporate risk management and path dependency. Trend-scanning adapts to varying market conditions. Meta-labeling optimizes conviction and position sizing. The method you choose fundamentally shapes what your model learns and how it behaves in live trading.

**Key Considerations when Working with Financial Data**

- Avoid Hindsight Bias:Â Ensure your labels are based only on information available up to the point of decision. For example, when labeling a data point for 'today', you can only use information from 'today' or earlier to determine features, and information from the 'future' (relative to 'today') to determine the label.
- Balance Your Classes: If you're labeling for 'buy', 'sell', and 'hold', you might find that 'hold' signals (or small movements) are far more frequent. Highly imbalanced classes can make it harder for ML models to learn the minority classes. Techniques exist to handle this (e.g., oversampling, undersampling, or using appropriate evaluation metrics).
- Volatility is Key: Financial markets have changing volatility. A 2% price move might be huge in a calm market but insignificant during a volatile period. Consider using volatility-adjusted thresholds for your labels (as in the Triple-Barrier Method).
- Stationarity: Financial time series are often non-stationary (their statistical properties like mean and variance change over time). While labeling itself doesn't directly make data stationary, the choice of labeling (e.g., labeling returns, which are often more stationary than prices) and subsequent feature engineering are crucial.
- Iterate and Refine: Your first labeling approach might not be the best. Be prepared to experiment with different methods, horizons, and thresholds to see what works best for your specific goals and the assets you're analysing.

### Implementation: Triple-Barrier Labeling Method

**Setting Dynamic Barriers**

As argued in the previous section, in practice we want to set profit taking and stop-loss limits that are a function of the risks involved in a bet. Otherwise, sometimes we will be aiming too high (ðœ â‰« ðœŽti,0), and sometimes too low (ðœ â‰ª ðœŽti,0), considering the prevailing volatility.

The code below calculates the daily volatility at intraday estimation points, applying a span of lookback days to an exponentially weighted moving standard deviation. This volatility will be used to set the profit-taking and stop-loss barriers.

```
def get_daily_vol(close, lookback=100):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Snippet 3.1, page 44.

Â Â Â Â Daily Volatility Estimates

Â Â Â Â Computes the daily volatility at intraday estimation points.

Â Â Â Â In practice we want to set profit taking and stop-loss limits that are a function of the risks involved
Â Â Â Â in a bet. Otherwise, sometimes we will be aiming too high (tao â‰« sigma_t_i,0), and sometimes too low
Â Â Â Â (tao â‰ª sigma_t_i,0), considering the prevailing volatility. Snippet 3.1 computes the daily volatility
Â Â Â Â at intraday estimation points, applying a span of lookback days to an exponentially weighted moving
Â Â Â Â standard deviation.

Â Â Â Â See the pandas documentation for details on the pandas.Series.ewm function.
Â Â Â Â Note: This function is used to compute dynamic thresholds for profit taking and stop loss limits.

Â Â Â Â :param close: (pd.Series) Closing prices
Â Â Â Â :param lookback: (int) Lookback period to compute volatility
Â Â Â Â :return: (pd.Series) Daily volatility value
Â Â Â Â """
Â Â Â Â # Find previous valid trading day for each date
Â Â Â Â prev_idx = close.index.searchsorted(close.index - pd.Timedelta(days=1))
Â Â Â Â prev_idx = prev_idx[prev_idx > 0]Â Â # Drop indices before the start

Â Â Â Â # Align current and previous closes
Â Â Â Â curr_idx = close.index[close.shape[0] - prev_idx.shape[0] :]
Â Â Â Â prev_close = close.iloc[prev_idx - 1].valuesÂ Â # Previous day's close
Â Â Â Â ret = close.loc[curr_idx] / prev_close - 1
Â Â Â Â vol = ret.ewm(span=lookback).std()
Â Â Â Â return vol
```

**Setting Vertical Barriers**

To set the vertical barriers we use the following function. When using activity-driven bars, it makes more sense to the barriers based on the number of bars to expiration rather than a fixed period as their can be extreme variability within a time horizon.

```
# Snippet 3.4 page 49, Adding a Vertical Barrier
def add_vertical_barrier(t_events, close, num_bars=0, **time_delta_kwargs):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Enhanced Implementation.

Â Â Â Â Adding a Vertical Barrier

Â Â Â Â For each event in t_events, finds the timestamp of the next price bar at or immediately after:
Â Â Â Â - A fixed number of bars (for activity-based sampling), OR
Â Â Â Â - A time delta (for time-based sampling)

Â Â Â Â This function creates a series of vertical barrier timestamps aligned with the original events index.
Â Â Â Â Out-of-bound barriers are marked with NaT for downstream handling.

Â Â Â Â :param t_events: (pd.Series) Series of event timestamps (e.g., from symmetric CUSUM filter)
Â Â Â Â :param close: (pd.Series) Close price series with DateTimeIndex
Â Â Â Â :param num_bars: (int) Number of bars for vertical barrier (activity-based mode).
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Takes precedence over time delta parameters when > 0.
Â Â Â Â :param time_delta_kwargs: Time components for time-based barrier (mutually exclusive with num_bars):
Â Â Â Â Â Â Â Â :param days: (int) Number of days
Â Â Â Â Â Â Â Â :param hours: (int) Number of hours
Â Â Â Â Â Â Â Â :param minutes: (int) Number of minutes
Â Â Â Â Â Â Â Â :param seconds: (int) Number of seconds
Â Â Â Â :return: (pd.Series) Vertical barrier timestamps with same index as t_events.
Â Â Â Â Â Â Â Â Â Â Â Â  Out-of-bound events return pd.NaT.

Â Â Â Â Example:
Â Â Â Â Â Â Â Â # Activity-bar mode (tick/volume/dollar bars)
Â Â Â Â Â Â Â Â vertical_barriers = add_vertical_barrier(t_events, close, num_bars=10)

Â Â Â Â Â Â Â Â # Time-based mode
Â Â Â Â Â Â Â Â vertical_barriers = add_vertical_barrier(t_events, close, days=1, hours=3)
Â Â Â Â """
Â Â Â Â # Validate inputs
Â Â Â Â if num_bars and time_delta_kwargs:
Â Â Â Â Â Â Â Â raise ValueError("Use either num_bars OR time deltas, not both")

Â Â Â Â # BAR-BASED VERTICAL BARRIERS
Â Â Â Â if num_bars > 0:
Â Â Â Â Â Â Â Â indices = close.index.get_indexer(t_events, method="nearest")
Â Â Â Â Â Â Â Â t1 = []
Â Â Â Â Â Â Â Â for i in indices:
Â Â Â Â Â Â Â Â Â Â Â Â if i == -1:Â Â # Event not found
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â t1.append(pd.NaT)
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â end_loc = i + num_bars
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â t1.append(close.index[end_loc] if end_loc < len(close) else pd.NaT)
Â Â Â Â Â Â Â Â return pd.Series(t1, index=t_events)

Â Â Â Â # TIME-BASED VERTICAL BARRIERS
Â Â Â Â td = pd.Timedelta(**time_delta_kwargs) if time_delta_kwargs else pd.Timedelta(0)
Â Â Â Â barrier_times = t_events + td

Â Â Â Â # Find next index positions
Â Â Â Â t1_indices = np.searchsorted(close.index, barrier_times, side="left")
Â Â Â Â t1 = []
Â Â Â Â for idx in t1_indices:
Â Â Â Â Â Â Â Â if idx < len(close):
Â Â Â Â Â Â Â Â Â Â Â Â t1.append(close.index[idx])
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â t1.append(pd.NaT)Â Â # Mark out-of-bound for downstream

Â Â Â Â return pd.Series(t1, index=t_events)
```

**Applying Triple-Barrier Labeling**

The triple-barrier method is inherently path-dependent, meaning we can't just look at the final price to determine our label. Instead, we need to track the entire price _journey_ from when we enter a position until we exit, whether that's due to hitting a profit target, stop loss, or time limit.

When we apply the triple-barrier method, we're essentially asking: "Starting from time _t_ _i,0_, what happens first as we move forward in time?" The process continues until we reach _t_ _i,1_, which represents the moment the first barrier gets touched. This could be our profit target, our stop loss, or our maximum holding period (the vertical barrier set at _t_ _i,0 + h_). The return we use for labeling is calculated from our entry point _t_ _i,0_ to this first touch time _ti,1_.

The functionÂ  _apply\_pt\_sl\_on\_t1()_Â implements this logic and requires several inputs to work properly. First, it needs the _close_Â price series so it can track the actual price path. The events Â dataframe contains the essential information for each potential trade: the t1 Â column specifies when each trade should expire (the vertical barrier), while the _trgt_ column defines how wide our horizontal barriers should be.

The _pt\_sl_ parameter is particularly important. It's a two-element list that controls the barrier widths. The first element ( _pt\_sl_\[0\]) sets how many multiples of the target width to use for the profit-taking barrier, while the second element ( _pt\_sl_\[1\]) does the same for the stop-loss barrier. If either value is zero, that particular barrier is disabled. This flexibility allows you to create asymmetric risk/reward setups. For example, you might set a tight stop loss but let profits run further.

The function processes these barriers systematically, tracking price movements bar by bar until one of the barriers is breached, giving us the precise timing and return information needed for accurate labeling.

```
# Snippet 3.2, page 45, Triple Barrier Labeling Method
def apply_pt_sl_on_t1(close, events, pt_sl, molecule):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Snippet 3.2, page 45.

Â Â Â Â Triple Barrier Labeling Method

Â Â Â Â This function applies the triple-barrier labeling method. It works on a set of
Â Â Â Â datetime index values (molecule). This allows the program to parallelize the processing.

Â Â Â Â Mainly it returns a DataFrame of timestamps regarding the time when the first barriers were reached.

Â Â Â Â :param close: (pd.Series) Close prices
Â Â Â Â :param events: (pd.Series) Indices that signify "events" (see cusum_filter function for more details)
Â Â Â Â :param pt_sl: (np.array) Element 0, indicates the profit taking level; Element 1 is stop loss level
Â Â Â Â :param molecule: (an array) A set of datetime index values for processing
Â Â Â Â :return: (pd.DataFrame) Timestamps of when first barrier was touched
Â Â Â Â """
Â Â Â Â # Apply stop loss/profit taking, if it takes place before t1 (end of event)
Â Â Â Â events = events.loc[molecule].copy()
Â Â Â Â out = events[["t1"]].copy(deep=True)

Â Â Â Â profit_taking_multiple = pt_sl[0]
Â Â Â Â stop_loss_multiple = pt_sl[1]

Â Â Â Â # Profit taking active
Â Â Â Â if profit_taking_multiple > 0:
Â Â Â Â Â Â Â Â profit_taking = np.log(1 + profit_taking_multiple * events["trgt"])
Â Â Â Â else:
Â Â Â Â Â Â Â Â profit_taking = pd.Series(index=events.index)Â Â # NaNs

Â Â Â Â # Stop loss active
Â Â Â Â if stop_loss_multiple > 0:
Â Â Â Â Â Â Â Â stop_loss = np.log(1 - stop_loss_multiple * events["trgt"])
Â Â Â Â else:
Â Â Â Â Â Â Â Â stop_loss = pd.Series(index=events.index)Â Â # NaNs

Â Â Â Â # Use dictionary to collect barrier hit times
Â Â Â Â barrier_dict = {"sl": {}, "pt": {}}

Â Â Â Â # Get events
Â Â Â Â for loc, vertical_barrier in events["t1"].fillna(close.index[-1]).items():
Â Â Â Â Â Â Â Â closing_prices = close[loc:vertical_barrier]Â Â # Path prices for a given trade
Â Â Â Â Â Â Â Â cum_returns = np.log(closing_prices / close[loc]) * events.at[loc, "side"]Â Â # Path returns
Â Â Â Â Â Â Â Â barrier_dict["sl"][loc] = cum_returns[\
Â Â Â Â Â Â Â Â Â Â Â Â cum_returns < stop_loss[loc]\
Â Â Â Â Â Â Â Â ].index.min()Â Â # Earliest stop loss date
Â Â Â Â Â Â Â Â barrier_dict["pt"][loc] = cum_returns[\
Â Â Â Â Â Â Â Â Â Â Â Â cum_returns > profit_taking[loc]\
Â Â Â Â Â Â Â Â ].index.min()Â Â # Earliest profit taking date

Â Â Â Â # Convert dictionary to DataFrame and join to `out`
Â Â Â Â barrier_df = pd.DataFrame(barrier_dict)
Â Â Â Â out = out.join(barrier_df)Â Â # Join on index (loc)

Â Â Â Â return out
```

The triple-barrier method offers eight possible configurations, depending on which barriers you activate. Think of each setup as \[profit\_target, stop\_loss, time\_limit\], where 1 means active and 0 means disabled.

- Most practical trading strategies use one of three configurations:

- \[1,1,1\] - The Complete Setup: All three barriers active. You're seeking profits while managing both downside risk and holding period. This mirrors how most professional traders actually operate.
- \[0,1,1\] - Let Winners Run: No profit target, but you'll exit after X periods unless stopped out first. Perfect for momentum strategies where you want to ride trends.
- \[1,1,0\] - No Time Pressure: Profit target and stop loss active, but no time limit. You'll hold until one of the price barriers is hit, however long that takes.

- Three technically possible but less realistic configurations:

  - \[0,0,1\] - Fixed Horizon: Just time-based exits. This is essentially fixed-time horizon labeling, though it can work with activity-driven bars.
  - \[1,0,1\] - Ignore Losses: Hold until profitable or time runs out, regardless of intermediate losses. Dangerous for risk management.
  - \[1,0,0\] - Hold Forever: No stop loss or time limit. Keep losing positions until they eventually become profitableâ€”a recipe for portfolio destruction.

- Two essentially useless configurations:

  - \[0,1,0\] - Expecting Failure: Hold positions until they hit the stop loss. Why enter trades expecting only losses?
  - \[0,0,0\] - No Exit: All barriers disabled. Positions never close and no labels are generated.

Below are two possible configurations of the triple-barrier method.

![](https://c.mql5.com/2/160/213812329077.png)

![](https://c.mql5.com/2/160/3603090486229.png)

**Learning for Side and Size**

The functions in this section utilize the triple-barrier method to either learn the side, {1, 0, -1}, whenÂ _side\_prediction_ =None in _get\_events()_, or to learn the size from the meta-labeled data when the side is known. Learning the side of the bet implies that there are no horizontal barriers,Â or thatÂ the horizontal barriers are symmetric. This is because we cannot differentiate between the profit-taking and stop-loss barriers at this point. Once side is known, we can optimize the barriers to find what combination results in the most performant model. We get the dates of the first barrier touched by runningÂ _get\_events()._

```
# Snippet 3.3 -> 3.6 page 50, Getting the Time of the First Touch, with Meta Labels
def get_events(close, t_events, pt_sl, target, min_ret, num_threads, vertical_barrier_times=False,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  side_prediction=None, verbose=True):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Snippet 3.6 page 50.

Â Â Â Â Getting the Time of the First Touch, with Meta Labels

Â Â Â Â This function is orchestrator to meta-label the data, in conjunction with the Triple Barrier Method.

Â Â Â Â :param close: (pd.Series) Close prices
Â Â Â Â :param t_events: (pd.Series) of t_events. These are timestamps that will seed every triple barrier.
Â Â Â Â Â Â Â Â These are the timestamps selected by the sampling procedures discussed in Chapter 2, Section 2.5.
Â Â Â Â Â Â Â Â E.g.: CUSUM Filter
Â Â Â Â :param pt_sl: (list) Element 0, indicates the profit taking level; Element 1 is stop loss level.
Â Â Â Â Â Â Â Â A non-negative float that sets the width of the two barriers. A 0 value means that the respective
Â Â Â Â Â Â Â Â horizontal barrier (profit taking and/or stop loss) will be disabled.
Â Â Â Â :param target: (pd.Series) of values that are used (in conjunction with pt_sl) to determine the width
Â Â Â Â Â Â Â Â of the barrier. In this program this is daily volatility series.
Â Â Â Â :param min_ret: (float) The minimum target return required for running a triple barrier search.
Â Â Â Â :param num_threads: (int) The number of threads concurrently used by the function.
Â Â Â Â :param vertical_barrier_times: (pd.Series) A pandas series with the timestamps of the vertical barriers.
Â Â Â Â Â Â Â Â We pass a False when we want to disable vertical barriers.
Â Â Â Â :param side_prediction: (pd.Series) Side of the bet (long/short) as decided by the primary model
Â Â Â Â :param verbose: (bool) Flag to report progress on asynch jobs
Â Â Â Â :return: (pd.DataFrame) Events
Â Â Â Â Â Â Â Â Â Â Â Â -events.index is event's starttime
Â Â Â Â Â Â Â Â Â Â Â Â -events['t1'] is event's endtime
Â Â Â Â Â Â Â Â Â Â Â Â -events['trgt'] is event's target
Â Â Â Â Â Â Â Â Â Â Â Â -events['side'] (optional) implies the algo's position side
Â Â Â Â Â Â Â Â Â Â Â Â -events['pt'] is profit taking multiple
Â Â Â Â Â Â Â Â Â Â Â Â -events['sl']Â Â is stop loss multiple
Â Â Â Â """

Â Â Â Â # 1) Get target
Â Â Â Â target = target.reindex(t_events)
Â Â Â Â target = target[target > min_ret]Â Â # min_ret

Â Â Â Â # 2) Get vertical barrier (max holding period)
Â Â Â Â if vertical_barrier_times is False:
Â Â Â Â Â Â Â Â vertical_barrier_times = pd.Series(pd.NaT, index=t_events, dtype=t_events.dtype)

Â Â Â Â # 3) Form events object, apply stop loss on vertical barrier
Â Â Â Â if side_prediction is None:
Â Â Â Â Â Â Â Â side_ = pd.Series(1.0, index=target.index)
Â Â Â Â Â Â Â Â pt_sl_ = [pt_sl[0], pt_sl[0]]
Â Â Â Â else:
Â Â Â Â Â Â Â Â side_ = side_prediction.reindex(target.index)Â Â # Subset side_prediction on target index.
Â Â Â Â Â Â Â Â pt_sl_ = pt_sl[:2]

Â Â Â Â # Create a new df with [v_barrier, target, side] and drop rows that are NA in target
Â Â Â Â events = pd.concat({'t1': vertical_barrier_times, 'trgt': target, 'side': side_}, axis=1)
Â Â Â Â events = events.dropna(subset=['trgt'])

Â Â Â Â # Apply Triple Barrier
Â Â Â Â first_touch_dates = mp_pandas_obj(func=apply_pt_sl_on_t1,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pd_obj=('molecule', events.index),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â num_threads=num_threads,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â close=close,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â events=events,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pt_sl=pt_sl_,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â verbose=verbose)

Â Â Â Â for ind in events.index:
Â Â Â Â Â Â Â Â events.at[ind, 't1'] = first_touch_dates.loc[ind, :].dropna().min()

Â Â Â Â if side_prediction is None:
Â Â Â Â Â Â Â Â events = events.drop('side', axis=1)

Â Â Â Â # Add profit taking and stop loss multiples for vertical barrier calculations
Â Â Â Â events['pt'] = pt_sl[0]
Â Â Â Â events['sl'] = pt_sl[1]

Â Â Â Â return events
```

We use _get\_bins()_Â  to return a dataframe _events_ where:

- _events_.index is event's start time
- _events_\['t1'\] is event's end time
- _events_\['trgt'\] is event's target
- _events_\['side'\] (optional) implies the strategy's position side

Labeling behavior depends on the presence of 'side':

- Case 1: If 'side' not in events â†’ bin âˆˆ {-1, 1} (label by price action)
- Case 2: If 'side' is presentÂ  Â  Â  â†’ bin âˆˆ {0, 1}Â  (label by PnL â€” meta-labeling)

```
# Snippet 3.4 -> 3.7, page 51, Labeling for Side & Size with Meta Labels
def get_bins(triple_barrier_events, close, vertical_barrier_zero=False, pt_sl=[1, 1]):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Snippet 3.7, page 51.

Â Â Â Â Labeling for Side & Size with Meta Labels

Â Â Â Â Compute event's outcome (including side information, if provided).
Â Â Â Â events is a DataFrame where:

Â Â Â Â Now the possible values for labels in out['bin'] are {0,1}, as opposed to whether to take the bet or pass,
Â Â Â Â a purely binary prediction. When the predicted label the previous feasible values {âˆ’1,0,1}.
Â Â Â Â The ML algorithm will be trained to decide is 1, we can use the probability of this secondary prediction
Â Â Â Â to derive the size of the bet, where the side (sign) of the position has been set by the primary model.

Â Â Â Â :param triple_barrier_events: (pd.DataFrame) Events DataFrame with the following structure:
Â Â Â Â - **index**: pd.DatetimeIndex of event start times
Â Â Â Â - **t1**: (pd.Series) Event end times
Â Â Â Â - **trgt**: (pd.Series) Target returns
Â Â Â Â - **side**: (pd.Series, optional) Algo's position side
Â Â Â Â Â Â Labeling behavior depends on the presence of 'side':
Â Â Â Â Â Â Â Â - Case 1: If 'side' not in events â†’ `bin âˆˆ {-1, 1}` (label by price action)
Â Â Â Â Â Â Â Â - Case 2: If 'side' is presentÂ Â Â Â â†’ `bin âˆˆ {0, 1}`Â Â (label by PnL â€” meta-labeling)
Â Â Â Â :param close: (pd.Series) Close prices
Â Â Â Â :param vertical_barrier_zero: (bool) If True, set bin to 0 for events that touch vertical barrier, else bin is the sign of the return.
Â Â Â Â :param pt_sl: (list) Take-profit and stop-loss multiples
Â Â Â Â :return: (pd.DataFrame) Meta-labeled events
Â Â Â Â :returns index: Event start times
Â Â Â Â :returns t1: Event end times
Â Â Â Â :returns trgt: Target returns
Â Â Â Â :returns side: Optional. Algo's position side
Â Â Â Â :returns ret: Returns of the event
Â Â Â Â :returns bin: Labels for the event, where 1 is a positive return, -1 is a negative return, and 0 is a vertical barrier hit
Â Â Â Â """

Â Â Â Â # 1. Align prices with their respective events
Â Â Â Â events = triple_barrier_events.dropna(subset=["t1"])
Â Â Â Â all_dates = events.index.union(other=events["t1"].array).drop_duplicates()
Â Â Â Â prices = close.reindex(all_dates, method="bfill")

Â Â Â Â # 2. Create out DataFrame
Â Â Â Â out_df = events[["t1"]].copy()
Â Â Â Â out_df["ret"] = np.log(prices.loc[events["t1"].array].array / prices.loc[events.index])
Â Â Â Â out_df["trgt"] = events["trgt"]

Â Â Â Â # Meta labeling: Events that were correct will have pos returns
Â Â Â Â if "side" in events:
Â Â Â Â Â Â Â Â out_df["ret"] *= events["side"]Â Â # meta-labeling

Â Â Â Â if vertical_barrier_zero:
Â Â Â Â Â Â Â Â # Label 0 when vertical barrier reached
Â Â Â Â Â Â Â Â out_df["bin"] = barrier_touched(
Â Â Â Â Â Â Â Â Â Â Â Â out_df["ret"].values,
Â Â Â Â Â Â Â Â Â Â Â Â out_df["trgt"].values,
Â Â Â Â Â Â Â Â Â Â Â Â np.array(pt_sl, dtype=float),
Â Â Â Â Â Â Â Â )
Â Â Â Â else:
Â Â Â Â Â Â Â Â # Label is the sign of the return
Â Â Â Â Â Â Â Â out_df["bin"] = np.where(out_df["ret"] > 0, 1, -1).astype("int8")

Â Â Â Â # Meta labeling: label incorrect events with a 0
Â Â Â Â if "side" in events:
Â Â Â Â Â Â Â Â out_df.loc[out_df["ret"] <= 0, "bin"] = 0

Â Â Â Â # Add the side to the output. This is useful for when a meta label model must be fit
Â Â Â Â if "side" in triple_barrier_events.columns:
Â Â Â Â Â Â Â Â out_df["side"] = triple_barrier_events["side"].astype("int8")

Â Â Â Â out_df["ret"] = np.exp(out_df["ret"]) - 1Â Â # Convert log returns to simple returns
Â Â Â Â return out_df
```

_NOTE_:Â _mp\_pandas\_obj()_ as used aboveÂ  is a helper function that facilitates parallel processing when working with pandas objects (pandas always runs on a single CPU). The attachmentÂ _multiprocess.py_Â contains this function, and other multiprocessing helpers.

To set the labelÂ to zero when a vertical barrier is touched, we callÂ  _barrier\_touched()_ in _get\_bins()_.

```
# Snippet 3.9, page 55, Question 3.3
def barrier_touched(ret, target, pt_sl):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Snippet 3.9, page 55, Question 3.3.

Â Â Â Â Adjust the getBins function (Snippet 3.7) to return a 0 whenever the vertical barrier is the one touched first.

Â Â Â Â Top horizontal barrier: 1
Â Â Â Â Bottom horizontal barrier: -1
Â Â Â Â Vertical barrier: 0

Â Â Â Â :param ret: (np.array) Log-returns
Â Â Â Â :param target: (np.array) Volatility target
Â Â Â Â :param pt_sl: (ArrayLike) Take-profit and stop-loss multiples
Â Â Â Â :return: (np.array) Labels
Â Â Â Â """
Â Â Â Â N = ret.shape[0]Â Â # Number of events
Â Â Â Â store = np.empty(N, dtype=np.int8)Â Â # Store labels in an array

Â Â Â Â profit_taking_multiple = pt_sl[0]
Â Â Â Â stop_loss_multiple = pt_sl[1]

Â Â Â Â # Iterate through the DataFrame and check if the vertical barrier was reached
Â Â Â Â for i in range(N):
Â Â Â Â Â Â Â Â pt_level_reached = ret[i] > np.log(1 + profit_taking_multiple * target[i])
Â Â Â Â Â Â Â Â sl_level_reached = ret[i] < np.log(1 - stop_loss_multiple * target[i])

Â Â Â Â Â Â Â Â if ret[i] > 0.0 and pt_level_reached:
Â Â Â Â Â Â Â Â Â Â Â Â # Top barrier reached
Â Â Â Â Â Â Â Â Â Â Â Â store[i] = 1
Â Â Â Â Â Â Â Â elif ret[i] < 0.0 and sl_level_reached:
Â Â Â Â Â Â Â Â Â Â Â Â # Bottom barrier reached
Â Â Â Â Â Â Â Â Â Â Â Â store[i] = -1
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â # Vertical barrier reached
Â Â Â Â Â Â Â Â Â Â Â Â store[i] = 0

Â Â Â Â return store
```

**When to Label as Zero orÂ as Sign of Return**

Labeling a nonâ€touch event as zero makes sense when you want your model to focus strictly onÂ clear riskâ€managed outcomes **,** whereas using the returnâ€™s sign at the horizon capturesÂ all directional drift, even if your risk thresholds are not breached.

Label as zero if:

- Youâ€™re building a threeâ€class classifier (up, down, neutral) and want â€œneutralâ€ to mean â€œno decisive moveâ€ within your risk limits.
- You prefer to filter outÂ ambiguous signalsâ€”small drifts that never touch either barrierâ€”from training.
- You want to measure event success purely by hitting your predefined profit or loss thresholds, ignoring all other cases.

Label as sign of return if:

- You need every sample to carry a binary direction (up/down) for a twoâ€class problem and want to avoid an over-abundance of neutral labels.
- Youâ€™re modelingÂ momentumÂ orÂ time-horizon returnsÂ rather than strictly risk-controlled exits.
- You believe price movementâ€”no matter how smallâ€”is informative and should influence your estimator.

Beyond this choice, consider how it affects class balance and label noise. If you see too many zeros, you might undersample neutrals or raise your horizontal barriers, and if your sign-of-return labels are too noisy, you could apply a minimum return threshold before assigning +1/âˆ’1.

**Event-Based Sampling**

As traders, we don't just randomly decide when to buy or sell securities. Instead, we wait for specific things to happen in the market before making their moves. These "trigger events" might include:

- When important economic data gets released (like employment numbers or inflation reports)
- When market prices suddenly become very unstable
- When the price difference between related investments moves far away from what's normally expected

Once one of these events happens, we treat it as a signal that something important might be occurring in the market. We can then let our ML algorithms figure out if their is an accurate prediction function under those circumstances.

The key idea is to learn whether there's an accurate way to predict market movements specifically when these events occur. If the algorithm shows that a particular type of event doesn't lead to accurate predictions, then we need to either change how we define what counts as a significant event, or try again using different features as inputs to the model.

**The CUSUM Filter**

A powerful technique for event-based sampling is theÂ CUSUM filter, a quality-control method used to detect shifts in the mean value of a measured quantity. In finance, we can adapt this filter to sample data points whenever a significant deviation in a market variable, like price, occurs. The CUSUM filter works by accumulating deviations from an expected value and triggering a sampling event when this accumulation surpasses a certain threshold.

The symmetric CUSUM filter is defined as follows:

- SâºÂ = max(0,Â SâºÂ + Î”P)

- Sâ» = min(0, Sâ» +Â Î”P)


WhereÂ Î”PÂ is the price change. An event is triggered when eitherÂ _Sâº_Â surpasses a positive thresholdÂ _h_Â orÂ _Sâ»_Â falls below a negative threshold - _h_. When an event is triggered, the corresponding accumulator is reset. This method avoids triggering multiple events when the price hovers around a threshold, a flaw common in popular market signals such as Bollinger bands. By using a CUSUM filter, we can create a feature matrix X that is sampled at moments of significant market activity, providing more relevant data for our ML models.

```
# Snippet 2.4, page 39, The Symmetric CUSUM Filter.
def cusum_filter(raw_time_series, threshold, time_stamps=True):
Â Â Â Â """
Â Â Â Â Advances in Financial Machine Learning, Snippet 2.4, page 39.

Â Â Â Â The Symmetric Dynamic/Fixed CUSUM Filter.

Â Â Â Â The CUSUM filter is a quality-control method, designed to detect a shift in the mean value of a measured quantity
Â Â Â Â away from a target value. The filter is set up to identify a sequence of upside or downside divergences from any
Â Â Â Â reset level zero. We sample a bar t if and only if S_t >= threshold, at which point S_t is reset to 0.

Â Â Â Â One practical aspect that makes CUSUM filters appealing is that multiple events are not triggered by raw_time_series
Â Â Â Â hovering around a threshold level, which is a flaw suffered by popular market signals such as Bollinger Bands.
Â Â Â Â It will require a full run of length threshold for raw_time_series to trigger an event.

Â Â Â Â Once we have obtained this subset of event-driven bars, we will let the ML algorithm determine whether the occurrence
Â Â Â Â of such events constitutes actionable intelligence. Below is an implementation of the Symmetric CUSUM filter.

Â Â Â Â Note: As per the book this filter is applied to closing prices but we extended it to also work on other
Â Â Â Â time series such as volatility.

Â Â Â Â :param raw_time_series: (pd.Series) Close prices (or other time series, e.g. volatility).
Â Â Â Â :param threshold: (float or pd.Series) When the abs(change) is larger than the threshold, the function captures
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â it as an event, can be dynamic if threshold is pd.Series
Â Â Â Â :param time_stamps: (bool) Default is to return a DateTimeIndex, change to false to have it return a list.
Â Â Â Â :return: (datetime index vector) Vector of datetimes when the events occurred. This is used later to sample.
Â Â Â Â """

Â Â Â Â t_events = []
Â Â Â Â s_pos = 0
Â Â Â Â s_neg = 0

Â Â Â Â # log returns
Â Â Â Â raw_time_series = pd.DataFrame(raw_time_series)Â Â # Convert to DataFrame
Â Â Â Â raw_time_series.columns = ['price']
Â Â Â Â raw_time_series['log_ret'] = raw_time_series.price.apply(np.log).diff()
Â Â Â Â if isinstance(threshold, (float, int)):
Â Â Â Â Â Â Â Â raw_time_series['threshold'] = threshold
Â Â Â Â elif isinstance(threshold, pd.Series):
Â Â Â Â Â Â Â Â raw_time_series.loc[threshold.index, 'threshold'] = threshold
Â Â Â Â else:
Â Â Â Â Â Â Â Â raise ValueError('threshold is neither float nor pd.Series!')

Â Â Â Â raw_time_series = raw_time_series.iloc[1:]Â Â # Drop first na values

Â Â Â Â # Get event time stamps for the entire series
Â Â Â Â for tup in raw_time_series.itertuples():
Â Â Â Â Â Â Â Â thresh = tup.threshold
Â Â Â Â Â Â Â Â pos = float(s_pos + tup.log_ret)
Â Â Â Â Â Â Â Â neg = float(s_neg + tup.log_ret)
Â Â Â Â Â Â Â Â s_pos = max(0.0, pos)
Â Â Â Â Â Â Â Â s_neg = min(0.0, neg)

Â Â Â Â Â Â Â Â if s_neg < -thresh:
Â Â Â Â Â Â Â Â Â Â Â Â s_neg = 0
Â Â Â Â Â Â Â Â Â Â Â Â t_events.append(tup.Index)

Â Â Â Â Â Â Â Â elif s_pos > thresh:
Â Â Â Â Â Â Â Â Â Â Â Â s_pos = 0
Â Â Â Â Â Â Â Â Â Â Â Â t_events.append(tup.Index)

Â Â Â Â # Return DatetimeIndex or list
Â Â Â Â if time_stamps:
Â Â Â Â Â Â Â Â event_timestamps = pd.DatetimeIndex(t_events)
Â Â Â Â Â Â Â Â return event_timestamps

Â Â Â Â return t_events
```

Let us analyze how a mean-reverting Bollinger band strategy performs when we use unfilteredÂ versus CUSUM-filtered entry signals.Â We will use EURUSD 5-minute time-bars from 2018-01-01 to 2021-12-31 for training and validation, and data from 2022-01-01 to 2024-12-31 for out-of-sample testing.

For the purposes of this demonstration, we will use a deliberately sensitive Bollinger Band configuration (20-period, 1.5 standard deviations) to generate a high volume of trade signals for the meta-model to evaluate. While this would be overly noisy for a standalone strategy, it provides an ideal stress test for our labeling and filtering pipeline.

To ensure consistency and interoperability across all strategies, I prefer to structure them using Pythonâ€™s object-oriented designâ€”specifically through a shared interface or base class. This approach allows each strategy to expose the same core functionality (e.g., signal generation, event filtering), making it easier to compare, extend, and integrate them within broader workflows. Using [TA-Lib](https://www.mql5.com/go?link=https://pypi.org/project/TA-Lib/ "https://pypi.org/project/TA-Lib/")Â and [Pandas TA](https://www.mql5.com/go?link=https://github.com/Data-Analisis/Technical-Analysis-Indicators---Pandas "https://github.com/Data-Analisis/Technical-Analysis-Indicators---Pandas")Â for the generation of commonly used signals is beneficial as it implements industry-standard formulas,Â is built on a highly optimized C backend, making it 2â€“4x faster than equivalent Python implementations, and ensures outputs are standardized and edge cases are handled consistently,Â which simplifies downstream tasks like labeling, filtering, or visualization. This library can sometimes be challenging to install, so I suggest following the instructions in this [article](https://www.mql5.com/go?link=https://www.geeksforgeeks.org/python/how-to-install-ta-lib-for-python/ "https://www.geeksforgeeks.org/python/how-to-install-ta-lib-for-python/").

```
import pandas as pd
from typing import Tuple, Union
import logging

from abc import ABC, abstractmethod
from typing import Dict, Tuple, Union

import numpy as np
import pandas as pd
import talib
from loguru import logger

class BaseStrategy(ABC):
Â Â Â Â """Abstract base class for trading strategies"""

Â Â Â Â @abstractmethod
Â Â Â Â def generate_signals(self, data: pd.DataFrame) -> pd.Series:
Â Â Â Â Â Â Â Â """Generate trading signals (1 for long, -1 for short, 0 for no position)"""
Â Â Â Â Â Â Â Â pass

Â Â Â Â @abstractmethod
Â Â Â Â def get_strategy_name(self) -> str:
Â Â Â Â Â Â Â Â """Return strategy name"""
Â Â Â Â Â Â Â Â pass

Â Â Â Â @abstractmethod
Â Â Â Â def get_objective(self) -> str:
Â Â Â Â Â Â Â Â """Return strategy objective"""
Â Â Â Â Â Â Â Â pass

class BollingerMeanReversionStrategy(BaseStrategy):
Â Â Â Â """Bollinger Bands mean reversion strategy"""

Â Â Â Â def __init__(self, window: int = 20, num_std: float = 2.0, objective: str = "mean_reversion"):
Â Â Â Â Â Â Â Â self.window = window
Â Â Â Â Â Â Â Â self.num_std = num_std
Â Â Â Â Â Â Â Â self.objective = objective

Â Â Â Â def generate_signals(self, data: pd.DataFrame) -> pd.Series:
Â Â Â Â Â Â Â Â """Generate mean-reversion signals using Bollinger Bands"""
Â Â Â Â Â Â Â Â close = data["close"]

Â Â Â Â Â Â Â Â # Calculate Bollinger Bands
Â Â Â Â Â Â Â Â upper_band, _, lower_band = talib.BBANDS(
Â Â Â Â Â Â Â Â Â Â Â Â close, timeperiod=self.window, nbdevup=self.num_std, nbdevdn=self.num_std
Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â # Generate signals
Â Â Â Â Â Â Â Â signals = pd.Series(0, index=data.index, dtype="int8", name="side")
Â Â Â Â Â Â Â Â signals[(close >= upper_band)] = -1Â Â # Sell signal (mean reversion)
Â Â Â Â Â Â Â Â signals[(close <= lower_band)] = 1Â Â # Buy signal (mean reversion)
Â Â Â Â Â Â Â Â return signals

Â Â Â Â def get_strategy_name(self) -> str:
Â Â Â Â Â Â Â Â return f"Bollinger_w{self.window}_std{self.num_std}"

Â Â Â Â def get_objective(self) -> str:
Â Â Â Â Â Â Â Â return self.objective

def get_entries(
Â Â Â Â strategy: 'BaseStrategy',
Â Â Â Â data: pd.DataFrame,
Â Â Â Â filter_events: bool = False,
Â Â Â Â filter_threshold: Union[float, pd.Series] = None,
Â Â Â Â on_crossover: bool = True,
) -> Tuple[pd.Series, pd.DatetimeIndex]:
Â Â Â Â """Get timestamps and position information for entry events.

Â Â Â Â This function processes signals from a given `BaseStrategy` to identify trade
Â Â Â Â entry points. It can apply a CUSUM filter to isolate significant events or,
Â Â Â Â by default, detect entries at signal crossover points.

Â Â Â Â Args:
Â Â Â Â Â Â Â Â strategy (BaseStrategy): The trading strategy object that generates the
Â Â Â Â Â Â Â Â Â Â Â Â primary signals.
Â Â Â Â Â Â Â Â data (pd.DataFrame): A pandas DataFrame containing the input data, expected
Â Â Â Â Â Â Â Â Â Â Â Â to have a 'close' column if `filter_events` is True.
Â Â Â Â Â Â Â Â filter_events (bool, optional): If True, a CUSUM filter is applied to the
Â Â Â Â Â Â Â Â Â Â Â Â signals to identify significant events. Defaults to False.
Â Â Â Â Â Â Â Â filter_threshold (Union[float, pd.Series], optional): The threshold for the
Â Â Â Â Â Â Â Â Â Â Â Â CUSUM filter. Must be a float or a pandas Series. Defaults to None.
Â Â Â Â Â Â Â Â on_crossover (bool, optional): If True, only events where the signal changes
Â Â Â Â Â Â Â Â Â Â Â Â from the previous period are considered entry points. Defaults to True.

Â Â Â Â Raises:
Â Â Â Â Â Â Â Â ValueError: If `filter_events` is True and `filter_threshold` is not a
Â Â Â Â Â Â Â Â Â Â Â Â `float` or `pd.Series`.

Â Â Â Â Returns:
Â Â Â Â Â Â Â Â Tuple[pd.Series, pd.DatetimeIndex]: A tuple containing:
Â Â Â Â Â Â Â Â Â Â Â Â side (pd.Series): A Series with the same index as the input data,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â where each value represents the trading position (-1 for short,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1 for long, 0 for no position).
Â Â Â Â Â Â Â Â Â Â Â Â t_events (pd.DatetimeIndex): A DatetimeIndex of the timestamps for
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â each detected entry event.
Â Â Â Â """
Â Â Â Â primary_signals = strategy.generate_signals(data)
Â Â Â Â signal_mask = primary_signals != 0

Â Â Â Â # Vectorized CUSUM filter application
Â Â Â Â if filter_events:
Â Â Â Â Â Â Â Â try:
Â Â Â Â Â Â Â Â Â Â Â Â close = data.close
Â Â Â Â Â Â Â Â except AttributeError as e:
Â Â Â Â Â Â Â Â Â Â Â Â logger.error(f"Dataframe must have a 'close' column: {e}")
Â Â Â Â Â Â Â Â Â Â Â Â raise e

Â Â Â Â Â Â Â Â if not isinstance(filter_threshold, (pd.Series, float)):
Â Â Â Â Â Â Â Â Â Â Â Â raise ValueError("filter_threshold must be a Series or a float")
Â Â Â Â Â Â Â Â elif isinstance(filter_threshold, pd.Series):
Â Â Â Â Â Â Â Â Â Â Â Â filter_threshold = filter_threshold.copy().dropna()
Â Â Â Â Â Â Â Â Â Â Â Â close = close.reindex(filter_threshold.index)

Â Â Â Â Â Â Â Â # Assuming cusum_filter is a function that takes a Series and a threshold
Â Â Â Â Â Â Â Â filtered_events = cusum_filter(close, filter_threshold)
Â Â Â Â Â Â Â Â signal_mask &= primary_signals.index.isin(filtered_events)
Â Â Â Â else:
Â Â Â Â Â Â Â Â # Vectorized signal change detection
Â Â Â Â Â Â Â Â if on_crossover:
Â Â Â Â Â Â Â Â Â Â Â Â signal_mask &= primary_signals != primary_signals.shift()

Â Â Â Â t_events = primary_signals.index[signal_mask]

Â Â Â Â side = pd.Series(index=data.index, name="side")
Â Â Â Â side.loc[t_events] = primary_signals.loc[t_events]
Â Â Â Â side = side.ffill().fillna(0).astype("int8")

Â Â Â Â if filter_events:
Â Â Â Â Â Â Â Â s = " generated by CUSUM filter"
Â Â Â Â elif on_crossover:
Â Â Â Â Â Â Â Â s = " generated by crossover"
Â Â Â Â else:
Â Â Â Â Â Â Â Â s = ""

Â Â Â Â logger.info(f"Generated {len(t_events):,} trade events{s}.")

Â Â Â Â return side, t_events
```

**Feature Engineering for Meta-Labeling**

To capture the nuanced dynamics of financial time series, we constructed a rich feature set that blends statistical rigor with domain-specific insight. This includes volatility-adjusted momentum indicators, autocorrelation structures, and higher-order return moments (skewness and kurtosis), alongside Bollinger-derived metrics and normalized moving average differentials. Technical signals such as RSI, MACD, ADX, and ATR were extracted using TA-Lib and pandas-ta, ensuring consistency and reproducibility. By incorporating both short-term and long-horizon featuresâ€”spanning lagged returns, Yang-Zhang volatility estimates, and trend-following indicatorsâ€”the model is equipped to detect subtle shifts in market behavior and respond to evolving trading conditions. The code can be found in the attachment features.py.

To illustrate the effectiveness of the CUSUM-filter, I trained a random forest using unfiltered and filtered signalsÂ on our EURUSD M5 data.Â My horizontal barriers were set using the exponentially weighted daily volatility over 100 days as my target, and setting the profit-taking barrier to 1 and the stop-loss barrier to 2. My vertical barrier was set to 50 bars.

target = get\_daily\_vol(close, lookback=100)

cusum\_filter\_threshold = target.mean()

**Effects of Filtering Data**

Before diving into our model comparisons, hereâ€™s a quick primer on the key metrics reported in each classification table:

- Precision: Percentage of correct positive predictions out of all positive calls. High precision means few false alarms.

- Recall: Percentage of actual positives correctly identified. High recall means few missed events.

- F1-score: Accuracy may not be an adequate classification score for meta-labeling applications. Suppose that, after you apply meta-labeling, there are many more negative cases (label â€˜0â€™) than positive cases (label â€˜1â€™). Under that scenario, a classifier that predicts every case to be negative will achieve high accuracy, even though recall=0 and precision is undefined. The F1 score corrects for that flaw, by assessing the classifier in terms of the (equally weighted) harmonic mean of precision and recall.

- Support: The number of instances for each class in the validation set.

- Accuracy:Overall fraction of correct predictions across all classes.

For an in-depth walkthrough of these metrics, see " [How to Interpret the Classification Report in sklearn](https://www.mql5.com/go?link=https://www.statology.org/sklearn-classification-report/ "https://www.statology.org/sklearn-classification-report/")".

Before we compare any scores, note how the CUSUM filter slashes sample size by 76.1%.

| UnfilteredÂ Signals | FilteredÂ Signals | Reduction % |
| --- | --- | --- |
| 32828 | 7825 | 76.1 |

_**TABLE 0:** Signal Count - Filtered vs. Unf _iltered__

The CUSUM filter aims to discard "noise," which is intrinsically linked toÂ class 0 dynamics. The ideal outcome is an improved F1 for class 1 (better event capture), while maintaining F1 for class 0 (no added false signals). Our filtered primary model is ideal, but our filtered meta-model experiences a 5.8% decrease in the F1 for class 0. To mitigate this, we could tune the hyper-parameters used in the model, or alter our feature set.

| Metric | Unfiltered | Filtered | Î” |
| --- | --- | --- | --- |
| Recall (1) | 1.00 | 1.00 | 0% |
| Precision (1) | 0.33 | 0.38 | +15.2% |
| F1 (1) | 0.49 | 0.55 | +11.0% |

_**Table**__**3:** Comparison of Filtered and UnfilteredPrimary-Models_

| Metric | Unfiltered | Filtered | Î” |
| --- | --- | --- | --- |
| F1 (0) | 0.69 | 0.65 | -5.8% |
| F1 (1) | 0.57 | 0.60 | +5.3% |
| Accuracy | 0.64 | 0.63 | -1.6% |

_**TableÂ 4**_ _**:**Comparison of Filtered and UnfilteredMeta-Models_

The trade-off between data reduction and model performance can be calculated as:

- _performance retention =_ _filtered performance / unfiltered performance \* 100_
- _net benefit = performance retention - data reduction_

If we obtain a positive net benefit, then the filter is efficient and should be used.

| Model | Data Reduction | Performance Retention | Net Benefit |
| --- | --- | --- | --- |
| Primary | 76.1% | 112.2% | +36.1% |
| Meta | 76.1% | 98.5% | +22.4% |

_**Table 5:** Trade-off Analysis_

We gain the following insights from comparing the unfiltered and filtered signals:

- The CUSUM filter successfully discarded 76.1% of events as noise.
- Filtering improved precision in both the primary model and meta-model.

- Filtering only slightly degraded overall performance of the meta-model despite 76%Â less data.Â There was a 5.3% gain for class 1, and a 5.6% drop for class 0, and despite the drop in class 0 F1, we still realised a net benefit of from using the CUSUM filter.

### Meta-Labeling Reports: Bollinger Band Strategy

The classification reports and ROC curves below underscore a key insight: meta-labeling thrives when paired with realistic, risk-aware labeling schemes.

![Classification Report: Fixed-Time Horizon Primary Mode;](https://c.mql5.com/2/165/fixed_time_primary_classification_report.png)

_**Figure 1**: _Fixed-Time Horizon_Primary ModelÂ Classification Report_

![Classification Report: Fixed-Time Horizon Meta-Model](https://c.mql5.com/2/165/fixed_time_meta_classification_report.png)

_**Figure 2**: _Fixed-Time Horizon_ Meta-Model _Classification Report__

![Classification Report: Triple-Barrier Primary Model](https://c.mql5.com/2/165/triple_barrier_primary_classification_report.png)

_**Figure 3**:Â _Triple-Barrier_ Primary Model Classification Report_

![Classification Report: Triple-Barrier Meta-Model](https://c.mql5.com/2/165/triple_barrier_meta_classification_report.png)

_**Figure 4**: _Triple-Barrier_ Meta-Model _Classification Report__

**Receiver Operating Characteristic Curves**

![ROC Curves: Fixed-Time Horizon vs Triple-Barrier Labels](https://c.mql5.com/2/165/roc_fixed_vs_triple_barrier.png)

_**Figure 5**: ROC for __Fixed-Time Horizon and__ Triple-Barrier Meta-Models_

**Result Interpretation: Decoding the Model's Performance**

The stark contrast in these results validates the core thesis: labeling defines success. The Fixed-Time Horizon model's classification report reveals a fundamental failure to learn; its inability to distinguish signals above randomness is graphically confirmed by its ROC curve clinging to the diagonal, a hallmark of a useless classifier. In contrast, the Triple-Barrier method's report shows a model that has successfully identified predictive patterns, a fact underscored by its ROC curve's pronounced bow towards the top-left corner, indicating a significant trade-off between true and false positive rates. This ROC AUC value, significantly above 0.5, demonstrates the model's tangible predictive power.

More importantly, the precision metrics translate directly to trading reality: a higher precision for the '1' class means a greater proportion of our predicted profitable trades (meta-labels) are likely to be correct, thereby increasing the potential profitability of the strategy and providing a concrete statistical foundation for confidence-based position sizing.

**Economic Significance:Â Out-of-SampleÂ Performance**

While the Triple-Barrier model showed a statistically significant improvement in AUC, its true value is revealed in the simulated equity curve. Figure 7 shows that the strategy leveraging our new labeling method achieved a -0.03% return with a maximum drawdown of 36.9%, significantly outperforming the benchmark Fixed-Horizon strategy, which achieved a -0.71% return with a maximum drawdown of 76%.

![Fixed-Time Horizon Strategy Performance](https://c.mql5.com/2/165/fixed_time_equity_curve.png)

_**Figure 6**:_ _Fixed-Time Horizon_ _PerformanceÂ Equity Curve_

![Triple-Barrier Strategy Performance](https://c.mql5.com/2/165/triple_barrier_equity_curve.png)

_**Figure 7**: Triple-Barrier Strategy PerformanceÂ Equity Curve_

Below is a comprehensive table of the performance metrics for each labeling method:

| Metrics | Fixed-Time Horizon | Triple-Barrier |
| --- | --- | --- |
| total\_return | -0.709771 | -0.028839 |
| annualized\_return | -0.338102 | -0.009714 |
| volatility | 0.483111 | 0.37613 |
| downside\_volatility | 0.336945 | 0.231413 |
| sharpe\_ratio | -4.778646 | -0.021566 |
| sortino\_ratio | -6.851611 | -0.035053 |
| var\_95 | -0.002864 | -0.00215 |
| cvar\_95 | -0.004164 | -0.002992 |
| skewness | -0.014451 | 0.034745 |
| kurtosis | 3.857222 | 2.507046 |
| max\_drawdown | 0.761708 | 0.368585 |
| avg\_drawdown | 0.08375 | 0.039945 |
| drawdown\_duration | 84 days 01:18:50 | 32 days 03:17:12 |
| ulcer\_index | 0.217503 | 0.098507 |
| calmar\_ratio | -0.443874 | -0.026354 |
| bet\_frequency | 3901 | 3969 |
| bets\_per\_year | 1300.040115 | 1322.701671 |
| num\_trades | 37691 | 27426 |
| trades\_per\_year | 12560.83363 | 9139.93853 |
| win\_rate | 0.497546 | 0.504339 |
| avg\_win | 0.001266 | 0.001081 |
| avg\_loss | -0.001322 | -0.001105 |
| best\_trade | 0.014599 | 0.01451 |
| worst\_trade | -0.013828 | -0.010548 |
| profit\_factor | 0.952754 | 0.999799 |
| expectancy | -0.000034 | -0.000002 |
| kelly\_criterion | -0.027194 | -0.002226 |
| consecutive\_wins | 77 | 92 |
| consecutive\_losses | 66 | 90 |
| avg\_trade\_duration | 0 days 00:39:18 | 0 days 06:22:15 |

_**Table 6**: Out-of-sample Performance Metrics_

### Conclusion

In this second installment of the MetaTrader 5 Machine Learning Blueprint, weâ€™ve explored how labeling choices shape the behavior and reliability of financial models. By moving beyond fixed-time horizons and embracing path-dependent techniques like the triple-barrier method, weâ€™ve shown how to encode risk-awareness and realistic trade dynamics directly into the learning process.

Meta-labeling emerged as a strategic overlay that filters low-conviction signals and enhances precisionâ€”especially when paired with robust labeling schemes. The classification reports and ROC curves demonstrate how this layered approach improves signal quality, even under aggressive filtering.

But our journey is far from over.

In the next article, weâ€™ll implement the trend-scanning method **,** which allows models to dynamically select their prediction horizon based on statistically significant price movements. This unlocks a new level of adaptability in volatile markets.

Weâ€™ll also tackle the challenge of concurrency in financial dataâ€”where multiple signals overlap in timeâ€”by introducing sample weightsÂ that reflect the uniqueness and relevance of each observation. This ensures that our models learn from truly independent signals, not redundant noise.

Finally, weâ€™ll explore how to leverage the probabilities generated by meta-labeling to size bets more intelligently. Instead of binary execution, weâ€™ll use probabilistic confidence to scale position sizesâ€”aligning model conviction with capital allocation.

Together, these enhancements will bring us closer to a production-grade machine learning pipeline for financial marketsâ€”one thatâ€™s not only technically sound, but strategically aligned with real-world trading behavior.

**Attached files** \|


[Download ZIP](https://www.mql5.com/en/articles/download/18864.zip "Download all attachments in the single ZIP archive")

[multiprocess.py](https://www.mql5.com/en/articles/download/18864/multiprocess.py "Download multiprocess.py")(9 KB)

[features.py](https://www.mql5.com/en/articles/download/18864/features.py "Download features.py")(12.35 KB)

**Warning:** All rights to these materials are reserved by MetaQuotes Ltd. Copying or reprinting of these materials in whole or in part is prohibited.

This article was written by a user of the site and reflects their personal views. MetaQuotes Ltd is not responsible for the accuracy of the information presented, nor for any consequences resulting from the use of the solutions, strategies or recommendations described.

#### Other articles by this author

- [MetaTrader 5 Machine Learning Blueprint (Part 6): Engineering a Production-Grade Caching System](https://www.mql5.com/en/articles/20302)
- [MetaTrader 5 Machine Learning Blueprint (Part 5): Sequential Bootstrappingâ€”Debiasing Labels, Improving Returns](https://www.mql5.com/en/articles/20059)
- [Machine Learning Blueprint (Part 4): The Hidden Flaw in Your Financial ML Pipeline â€” Label Concurrency](https://www.mql5.com/en/articles/19850)
- [MetaTrader 5 Machine Learning Blueprint (Part 3): Trend-Scanning Labeling Method](https://www.mql5.com/en/articles/19253)
- [MetaTrader 5 Machine Learning Blueprint (Part 1): Data Leakage and Timestamp Fixes](https://www.mql5.com/en/articles/17520)

**Last comments \|**
**[Go to discussion](https://www.mql5.com/en/forum/493807)**
(1)


![daxiritchietrade](https://c.mql5.com/avatar/avatar_na2.png)

**[daxiritchietrade](https://www.mql5.com/en/users/daxiritchietrade)**
\|
19 Dec 2025 at 04:44

Something I don't get:

If you train models with not the raw tick data but built bars(time, tick etc), do you have to build bars during live trading?

![From Basic to Intermediate: Template and Typename (IV)](https://c.mql5.com/2/114/Do_bgsico_ao_intermedikrio__Template_e_Typename_I___LOGO.png)[From Basic to Intermediate: Template and Typename (IV)](https://www.mql5.com/en/articles/15670)

In this article, we will take a very close look at how to solve the problem posed at the end of the previous article. There was an attempt to create a template of such type so that to be able to create a template for data union.

![MQL5 Wizard Techniques you should know (Part 79): Using Gator Oscillator and Accumulation/Distribution Oscillator with Supervised Learning](https://c.mql5.com/2/164/19220-mql5-wizard-techniques-you-logo.png)[MQL5 Wizard Techniques you should know (Part 79): Using Gator Oscillator and Accumulation/Distribution Oscillator with Supervised Learning](https://www.mql5.com/en/articles/19220)

In the last piece, we concluded our look at the pairing of the gator oscillator and the accumulation/distribution oscillator when used in their typical setting of the raw signals they generate. These two indicators are complimentary as trend and volume indicators, respectively. We now follow up that piece, by examining the effect that supervised learning can have on enhancing some of the feature patterns we had reviewed. Our supervised learning approach is a CNN that engages with kernel regression and dot product similarity to size its kernels and channels. As always, we do this in a custom signal class file that works with the MQL5 wizard to assemble an Expert Advisor.

![Reimagining Classic Strategies (Part 15): Daily Breakout Trading Strategy](https://c.mql5.com/2/165/19130-reimagining-classic-strategies-logo__1.png)[Reimagining Classic Strategies (Part 15): Daily Breakout Trading Strategy](https://www.mql5.com/en/articles/19130)

Human traders had long participated in financial markets before the rise of computers, developing rules of thumb that guided their decisions. In this article, we revisit a well-known breakout strategy to test whether such market logic, learned through experience, can hold its own against systematic methods. Our findings show that while the original strategy produced high accuracy, it suffered from instability and poor risk control. By refining the approach, we demonstrate how discretionary insights can be adapted into more robust, algorithmic trading strategies.

![Price Action Analysis Toolkit Development (Part 37): Sentiment Tilt Meter](https://c.mql5.com/2/165/19137-price-action-analysis-toolkit-logo.png)[Price Action Analysis Toolkit Development (Part 37): Sentiment Tilt Meter](https://www.mql5.com/en/articles/19137)

Market sentiment is one of the most overlooked yet powerful forces influencing price movement. While most traders rely on lagging indicators or guesswork, the Sentiment Tilt Meter (STM) EA transforms raw market data into clear, visual guidance, showing whether the market is leaning bullish, bearish, or staying neutral in real-time. This makes it easier to confirm trades, avoid false entries, and time market participation more effectively.

[We've created a channel for MQL5 developersFollow MQL5.community on social media and be the first to receive important updatesLearn more![](https://www.mql5.com/ff/sh/a83xrgctr82w45z9z2/01.png)](https://www.mql5.com/ff/go?link=https://www.mql5.com/en/forum/455636%3Futm_source=www.mql5.com%26utm_medium=display%26utm_content=follow.channel%26utm_campaign=AAA380.mql5.socials&a=pwgdbvtemvkwsfltqonysypfvtrtufji&s=e99a66a1660cd810b1edbac65597df695e2c2220d1e937834f402f9aeabd4289&uid=&ref=https://www.mql5.com/en/articles/18864&id=wdausxxqrpvhekbwjrjlhqjghyhesrqqau&fz_uniq=5049483154928020562)

![MQL5 - Language of trade strategies built-in the MetaTrader 5 client terminal](https://c.mql5.com/i/registerlandings/logo-2.png)

You are missing trading opportunities:

- Free trading apps
- Over 8,000 signals for copying
- Economic news for exploring financial markets

RegistrationLog in

latin characters without spaces

a password will be sent to this email

An error occurred


- [Log in With Google](https://www.mql5.com/en/auth_oauth2?provider=Google&amp;return=popup&amp;reg=1)

You agree to [website policy](https://www.mql5.com/en/about/privacy) and [terms of use](https://www.mql5.com/en/about/terms)

If you do not have an account, please [register](https://www.mql5.com/en/auth_register)

Allow the use of cookies to log in to the MQL5.com website.

Please enable the necessary setting in your browser, otherwise you will not be able to log in.

[Forgot your login/password?](https://www.mql5.com/en/auth_forgotten?return=popup)

- [Log in With Google](https://www.mql5.com/en/auth_oauth2?provider=Google&amp;return=popup)

This website uses cookies. Learn more about our [Cookies Policy](https://www.mql5.com/en/about/cookies).